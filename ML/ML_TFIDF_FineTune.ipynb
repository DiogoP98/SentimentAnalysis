{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "ML_TFIDF_FineTune.ipynb",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "KoghZU9Zu9EY",
        "pycharm": {
          "name": "#%%\n"
        },
        "outputId": "189588a2-d728-4f51-8a77-b3829ceb5347",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from imblearn.pipeline import Pipeline as imPipeline\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.metrics import classification_report, f1_score, roc_auc_score, accuracy_score\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
        "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from spacy.lang.en.stop_words import STOP_WORDS\n",
        "from sklearn.base import TransformerMixin\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import accuracy_score\n",
        "from IPython.display import display, HTML\n",
        "from scipy.stats import uniform\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/externals/six.py:31: FutureWarning: The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/).\n",
            "  \"(https://pypi.org/project/six/).\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.neighbors.base module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.neighbors. Anything that cannot be imported from sklearn.neighbors is now part of the private API.\n",
            "  warnings.warn(message, FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "hKSF3RsZu9EY",
        "pycharm": {
          "name": "#%%\n"
        },
        "outputId": "2145b1ef-265f-4521-8b69-44a303f466fb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "root_path = \"/content/drive/My Drive/notebooks\"\n",
        "# For Google colab only\n",
        "\n",
        "df = pd.read_csv(f\"{root_path}/new_clean_sm_100000.csv\")\n",
        "df = df[df['reviewText'].notna()]\n",
        "df = df[~df['reviewText'].str.contains(\".jpg|.png|.jpeg|.tiff|.gif|.bmp|.heif\", regex=True, na=False)]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "bsMDuk3Eu9F8",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "Split data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "hT55436Ju9F8",
        "pycharm": {
          "name": "#%%\n"
        },
        "colab": {}
      },
      "source": [
        "### Convert to a two class problem -  optional\n",
        "\n",
        "df = df[df['overall'] != 2]\n",
        "df = df[df['overall'] != 4]\n",
        "df.loc[df['overall'] == 1, 'overall'] = 0\n",
        "df.loc[df['overall'] == 3, 'overall'] = 1\n",
        "df.loc[df['overall'] == 5, 'overall'] = 2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ryK1dwGZu9F8",
        "pycharm": {
          "name": "#%%\n"
        },
        "colab": {}
      },
      "source": [
        "X = df.reviewText.values\n",
        "y = df.overall.values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "LcsFqUOlu9F8",
        "pycharm": {
          "name": "#%%\n"
        },
        "colab": {}
      },
      "source": [
        "num = len(df)\n",
        "X, y  = X[:num], y[:num]\n",
        "# le = LabelEncoder()\n",
        "# y = le.fit_transform(y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "WZjcShq9u9F8",
        "pycharm": {
          "name": "#%%\n"
        },
        "colab": {}
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "MhAPlC6Cu9Hg",
        "pycharm": {
          "name": "#%%\n"
        },
        "colab": {}
      },
      "source": [
        "def train_predict(pipline):\n",
        "    pipeline.fit(X_train, y_train)\n",
        "    y_pred = pipeline.predict(X_test)\n",
        "    score = classification_report(y_test, y_pred)\n",
        "    f1 = f1_score(y_test, y_pred, average='macro')\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    print(score)\n",
        "    print(f\"f1 score is {f1}, accuracy is {accuracy}\")\n",
        "\n",
        "def train_predict_all(pipeline):\n",
        "    out = pipeline.fit(X, y).cv_results_\n",
        "    results_df = pd.DataFrame({'rank': out['rank_test_score'],\n",
        "                          'params': out['params'],\n",
        "                           'cv score (mean)': out['mean_test_score']})\n",
        "    results_df = results_df.sort_values(by=['rank'], ascending=True)\n",
        "    pd.set_option('display.max_colwidth',100)\n",
        "    display(HTML(results_df.to_html()))\n",
        "    return out\n",
        "    # score = classification_report(y_test, y_pred)\n",
        "    # f1 = f1_score(y_test, y_pred, average='macro')\n",
        "    # accuracy = accuracy_score(y_test, y_pred)\n",
        "    # print(score)\n",
        "    # print(f\"f1 score is {f1}, accuracy is {accuracy}\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8NafI0YWYWz2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "OrHKgr4Qu9Hg",
        "pycharm": {
          "is_executing": false,
          "name": "#%% md\n"
        }
      },
      "source": [
        "Vectorizers\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Uq3xzk7ru9Hg",
        "pycharm": {
          "name": "#%%\n"
        },
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "count_vectoriser = Pipeline([\n",
        "                ('countVectoriser', CountVectorizer())\n",
        "            ])\n",
        "\n",
        "tfidf_vectoriser = Pipeline([\n",
        "                ('tfidfVectoriser', TfidfVectorizer(stop_words=STOP_WORDS, ngram_range = (1,2)\n",
        "                                                    ))\n",
        "            ])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "brx01xkqaW70"
      },
      "source": [
        "### Stantard Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "h0UeIUoqu9Hg",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "Naive Bayes -  Fine Tune\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "KLqrDXvZu9Hg",
        "pycharm": {
          "name": "#%%\n"
        },
        "colab": {}
      },
      "source": [
        "param_grid = {'vectoriser__tfidfVectoriser__ngram_range': [(1,1),(1,2)],\n",
        "              'classifier__classifier__alpha': [1e-5, 1e-4, 1e-2, 1e-1,1]}\n",
        "\n",
        "\n",
        "# param_grid = {'vectoriser__tfidfVectoriser__ngram_range': [(1,1)],\n",
        "#               'classifier__classifier__alpha': [1e-5]}\n",
        "\n",
        "naive_bayes = Pipeline([\n",
        "    ('classifier', MultinomialNB()),\n",
        "])\n",
        "\n",
        "pipeline = Pipeline([\n",
        "    ('vectoriser', tfidf_vectoriser),\n",
        "    ('classifier', naive_bayes)\n",
        "])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "NIZko0QUu9Hg",
        "outputId": "f8c07aca-0fae-4ad9-b1aa-9f7ac2c2ef98",
        "pycharm": {
          "name": "#%%\n"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 280
        }
      },
      "source": [
        "gs_mnb = GridSearchCV(pipeline, param_grid, cv=5, verbose=2, n_jobs=3)\n",
        "final_results = train_predict_all(gs_mnb)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n",
            "[Parallel(n_jobs=3)]: Done  35 tasks      | elapsed:  7.8min\n",
            "[Parallel(n_jobs=3)]: Done  50 out of  50 | elapsed: 12.0min finished\n",
            "/home/alex/anaconda3/envs/deepLearning/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n",
            "  'stop_words.' % sorted(inconsistent))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>rank</th>\n",
              "      <th>params</th>\n",
              "      <th>cv score (mean)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>1</td>\n",
              "      <td>{'classifier__classifier__alpha': 1, 'vectoriser__tfidfVectoriser__ngram_range': (1, 2)}</td>\n",
              "      <td>0.782966</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>2</td>\n",
              "      <td>{'classifier__classifier__alpha': 0.1, 'vectoriser__tfidfVectoriser__ngram_range': (1, 2)}</td>\n",
              "      <td>0.778564</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>3</td>\n",
              "      <td>{'classifier__classifier__alpha': 0.01, 'vectoriser__tfidfVectoriser__ngram_range': (1, 2)}</td>\n",
              "      <td>0.759132</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>4</td>\n",
              "      <td>{'classifier__classifier__alpha': 1, 'vectoriser__tfidfVectoriser__ngram_range': (1, 1)}</td>\n",
              "      <td>0.752286</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>5</td>\n",
              "      <td>{'classifier__classifier__alpha': 0.1, 'vectoriser__tfidfVectoriser__ngram_range': (1, 1)}</td>\n",
              "      <td>0.745731</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>6</td>\n",
              "      <td>{'classifier__classifier__alpha': 0.01, 'vectoriser__tfidfVectoriser__ngram_range': (1, 1)}</td>\n",
              "      <td>0.737958</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>7</td>\n",
              "      <td>{'classifier__classifier__alpha': 0.0001, 'vectoriser__tfidfVectoriser__ngram_range': (1, 1)}</td>\n",
              "      <td>0.726656</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>8</td>\n",
              "      <td>{'classifier__classifier__alpha': 1e-05, 'vectoriser__tfidfVectoriser__ngram_range': (1, 1)}</td>\n",
              "      <td>0.723352</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>9</td>\n",
              "      <td>{'classifier__classifier__alpha': 0.0001, 'vectoriser__tfidfVectoriser__ngram_range': (1, 2)}</td>\n",
              "      <td>0.722207</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>10</td>\n",
              "      <td>{'classifier__classifier__alpha': 1e-05, 'vectoriser__tfidfVectoriser__ngram_range': (1, 2)}</td>\n",
              "      <td>0.710515</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nXvSgmhPpEqk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "PfHaBUJUu9Hg",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "Logistic Regression - Fine Tune"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-8wx5ysOu9Hg",
        "pycharm": {
          "name": "#%%\n"
        },
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "param_grid = {'vectoriser__tfidfVectoriser__ngram_range': [(1,1),(1,2)],\n",
        "              'classifier__classifier__C': np.logspace(0, 4, num=3),\n",
        "              'classifier__classifier__penalty': ['l1', 'l2']}\n",
        "\n",
        "logistic_regression = Pipeline([\n",
        "    ('classifier', LogisticRegression(n_jobs=-1)),\n",
        "])\n",
        "\n",
        "pipeline = Pipeline([\n",
        "    ('vectoriser', tfidf_vectoriser),\n",
        "    ('classifier', logistic_regression)\n",
        "])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "1Zy17E72u9Hg",
        "outputId": "77f2d94a-599f-435f-95cb-09be27483c18",
        "pycharm": {
          "name": "#%%\n"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "gs_mnb = RandomizedSearchCV(pipeline, param_grid, cv=5, verbose=2)\n",
        "results = train_predict_all(gs_mnb)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
            "[CV] vectoriser__tfidfVectoriser__ngram_range=(1, 1), classifier__classifier__penalty=l2, classifier__classifier__C=100.0 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n",
            "  'stop_words.' % sorted(inconsistent))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  vectoriser__tfidfVectoriser__ngram_range=(1, 1), classifier__classifier__penalty=l2, classifier__classifier__C=100.0, total=  51.8s\n",
            "[CV] vectoriser__tfidfVectoriser__ngram_range=(1, 1), classifier__classifier__penalty=l2, classifier__classifier__C=100.0 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   51.8s remaining:    0.0s\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n",
            "  'stop_words.' % sorted(inconsistent))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  vectoriser__tfidfVectoriser__ngram_range=(1, 1), classifier__classifier__penalty=l2, classifier__classifier__C=100.0, total=  51.4s\n",
            "[CV] vectoriser__tfidfVectoriser__ngram_range=(1, 1), classifier__classifier__penalty=l2, classifier__classifier__C=100.0 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n",
            "  'stop_words.' % sorted(inconsistent))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  vectoriser__tfidfVectoriser__ngram_range=(1, 1), classifier__classifier__penalty=l2, classifier__classifier__C=100.0, total=  51.5s\n",
            "[CV] vectoriser__tfidfVectoriser__ngram_range=(1, 1), classifier__classifier__penalty=l2, classifier__classifier__C=100.0 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n",
            "  'stop_words.' % sorted(inconsistent))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  vectoriser__tfidfVectoriser__ngram_range=(1, 1), classifier__classifier__penalty=l2, classifier__classifier__C=100.0, total=  51.1s\n",
            "[CV] vectoriser__tfidfVectoriser__ngram_range=(1, 1), classifier__classifier__penalty=l2, classifier__classifier__C=100.0 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n",
            "  'stop_words.' % sorted(inconsistent))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  vectoriser__tfidfVectoriser__ngram_range=(1, 1), classifier__classifier__penalty=l2, classifier__classifier__C=100.0, total=  50.1s\n",
            "[CV] vectoriser__tfidfVectoriser__ngram_range=(1, 2), classifier__classifier__penalty=l1, classifier__classifier__C=100.0 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n",
            "  'stop_words.' % sorted(inconsistent))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
            "\n",
            "  FitFailedWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  vectoriser__tfidfVectoriser__ngram_range=(1, 2), classifier__classifier__penalty=l1, classifier__classifier__C=100.0, total= 1.0min\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n",
            "  'stop_words.' % sorted(inconsistent))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n",
            "  'stop_words.' % sorted(inconsistent))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV] vectoriser__tfidfVectoriser__ngram_range=(1, 2), classifier__classifier__penalty=l1, classifier__classifier__C=100.0 \n",
            "[CV]  vectoriser__tfidfVectoriser__ngram_range=(1, 2), classifier__classifier__penalty=l1, classifier__classifier__C=100.0, total= 1.0min\n",
            "[CV] vectoriser__tfidfVectoriser__ngram_range=(1, 2), classifier__classifier__penalty=l1, classifier__classifier__C=100.0 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
            "\n",
            "  FitFailedWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  vectoriser__tfidfVectoriser__ngram_range=(1, 2), classifier__classifier__penalty=l1, classifier__classifier__C=100.0, total= 1.0min\n",
            "[CV] vectoriser__tfidfVectoriser__ngram_range=(1, 2), classifier__classifier__penalty=l1, classifier__classifier__C=100.0 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n",
            "  'stop_words.' % sorted(inconsistent))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
            "\n",
            "  FitFailedWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  vectoriser__tfidfVectoriser__ngram_range=(1, 2), classifier__classifier__penalty=l1, classifier__classifier__C=100.0, total= 1.0min\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n",
            "  'stop_words.' % sorted(inconsistent))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
            "\n",
            "  FitFailedWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV] vectoriser__tfidfVectoriser__ngram_range=(1, 2), classifier__classifier__penalty=l1, classifier__classifier__C=100.0 \n",
            "[CV]  vectoriser__tfidfVectoriser__ngram_range=(1, 2), classifier__classifier__penalty=l1, classifier__classifier__C=100.0, total= 1.0min\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n",
            "  'stop_words.' % sorted(inconsistent))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n",
            "  'stop_words.' % sorted(inconsistent))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV] vectoriser__tfidfVectoriser__ngram_range=(1, 1), classifier__classifier__penalty=l1, classifier__classifier__C=1.0 \n",
            "[CV]  vectoriser__tfidfVectoriser__ngram_range=(1, 1), classifier__classifier__penalty=l1, classifier__classifier__C=1.0, total=  15.6s\n",
            "[CV] vectoriser__tfidfVectoriser__ngram_range=(1, 1), classifier__classifier__penalty=l1, classifier__classifier__C=1.0 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n",
            "  'stop_words.' % sorted(inconsistent))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  vectoriser__tfidfVectoriser__ngram_range=(1, 1), classifier__classifier__penalty=l1, classifier__classifier__C=1.0, total=  15.7s\n",
            "[CV] vectoriser__tfidfVectoriser__ngram_range=(1, 1), classifier__classifier__penalty=l1, classifier__classifier__C=1.0 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n",
            "  'stop_words.' % sorted(inconsistent))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  vectoriser__tfidfVectoriser__ngram_range=(1, 1), classifier__classifier__penalty=l1, classifier__classifier__C=1.0, total=  15.7s\n",
            "[CV] vectoriser__tfidfVectoriser__ngram_range=(1, 1), classifier__classifier__penalty=l1, classifier__classifier__C=1.0 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n",
            "  'stop_words.' % sorted(inconsistent))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  vectoriser__tfidfVectoriser__ngram_range=(1, 1), classifier__classifier__penalty=l1, classifier__classifier__C=1.0, total=  15.6s\n",
            "[CV] vectoriser__tfidfVectoriser__ngram_range=(1, 1), classifier__classifier__penalty=l1, classifier__classifier__C=1.0 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n",
            "  'stop_words.' % sorted(inconsistent))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  vectoriser__tfidfVectoriser__ngram_range=(1, 1), classifier__classifier__penalty=l1, classifier__classifier__C=1.0, total=  15.7s\n",
            "[CV] vectoriser__tfidfVectoriser__ngram_range=(1, 1), classifier__classifier__penalty=l1, classifier__classifier__C=100.0 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n",
            "  'stop_words.' % sorted(inconsistent))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  vectoriser__tfidfVectoriser__ngram_range=(1, 1), classifier__classifier__penalty=l1, classifier__classifier__C=100.0, total=  15.5s\n",
            "[CV] vectoriser__tfidfVectoriser__ngram_range=(1, 1), classifier__classifier__penalty=l1, classifier__classifier__C=100.0 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n",
            "  'stop_words.' % sorted(inconsistent))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  vectoriser__tfidfVectoriser__ngram_range=(1, 1), classifier__classifier__penalty=l1, classifier__classifier__C=100.0, total=  15.7s\n",
            "[CV] vectoriser__tfidfVectoriser__ngram_range=(1, 1), classifier__classifier__penalty=l1, classifier__classifier__C=100.0 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n",
            "  'stop_words.' % sorted(inconsistent))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  vectoriser__tfidfVectoriser__ngram_range=(1, 1), classifier__classifier__penalty=l1, classifier__classifier__C=100.0, total=  15.5s\n",
            "[CV] vectoriser__tfidfVectoriser__ngram_range=(1, 1), classifier__classifier__penalty=l1, classifier__classifier__C=100.0 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n",
            "  'stop_words.' % sorted(inconsistent))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  vectoriser__tfidfVectoriser__ngram_range=(1, 1), classifier__classifier__penalty=l1, classifier__classifier__C=100.0, total=  15.6s\n",
            "[CV] vectoriser__tfidfVectoriser__ngram_range=(1, 1), classifier__classifier__penalty=l1, classifier__classifier__C=100.0 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n",
            "  'stop_words.' % sorted(inconsistent))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  vectoriser__tfidfVectoriser__ngram_range=(1, 1), classifier__classifier__penalty=l1, classifier__classifier__C=100.0, total=  15.7s\n",
            "[CV] vectoriser__tfidfVectoriser__ngram_range=(1, 2), classifier__classifier__penalty=l2, classifier__classifier__C=1.0 \n",
            "[CV]  vectoriser__tfidfVectoriser__ngram_range=(1, 2), classifier__classifier__penalty=l2, classifier__classifier__C=1.0, total= 6.8min\n",
            "[CV] vectoriser__tfidfVectoriser__ngram_range=(1, 2), classifier__classifier__penalty=l2, classifier__classifier__C=1.0 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n",
            "  'stop_words.' % sorted(inconsistent))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  vectoriser__tfidfVectoriser__ngram_range=(1, 2), classifier__classifier__penalty=l2, classifier__classifier__C=1.0, total= 6.7min\n",
            "[CV] vectoriser__tfidfVectoriser__ngram_range=(1, 2), classifier__classifier__penalty=l2, classifier__classifier__C=1.0 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n",
            "  'stop_words.' % sorted(inconsistent))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  vectoriser__tfidfVectoriser__ngram_range=(1, 2), classifier__classifier__penalty=l2, classifier__classifier__C=1.0, total= 6.8min\n",
            "[CV] vectoriser__tfidfVectoriser__ngram_range=(1, 2), classifier__classifier__penalty=l2, classifier__classifier__C=1.0 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n",
            "  'stop_words.' % sorted(inconsistent))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  vectoriser__tfidfVectoriser__ngram_range=(1, 2), classifier__classifier__penalty=l2, classifier__classifier__C=1.0, total= 6.7min\n",
            "[CV] vectoriser__tfidfVectoriser__ngram_range=(1, 2), classifier__classifier__penalty=l2, classifier__classifier__C=1.0 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n",
            "  'stop_words.' % sorted(inconsistent))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  vectoriser__tfidfVectoriser__ngram_range=(1, 2), classifier__classifier__penalty=l2, classifier__classifier__C=1.0, total= 6.8min\n",
            "[CV] vectoriser__tfidfVectoriser__ngram_range=(1, 1), classifier__classifier__penalty=l2, classifier__classifier__C=1.0 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n",
            "  'stop_words.' % sorted(inconsistent))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  vectoriser__tfidfVectoriser__ngram_range=(1, 1), classifier__classifier__penalty=l2, classifier__classifier__C=1.0, total= 1.0min\n",
            "[CV] vectoriser__tfidfVectoriser__ngram_range=(1, 1), classifier__classifier__penalty=l2, classifier__classifier__C=1.0 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n",
            "  'stop_words.' % sorted(inconsistent))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  vectoriser__tfidfVectoriser__ngram_range=(1, 1), classifier__classifier__penalty=l2, classifier__classifier__C=1.0, total=  51.7s\n",
            "[CV] vectoriser__tfidfVectoriser__ngram_range=(1, 1), classifier__classifier__penalty=l2, classifier__classifier__C=1.0 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n",
            "  'stop_words.' % sorted(inconsistent))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  vectoriser__tfidfVectoriser__ngram_range=(1, 1), classifier__classifier__penalty=l2, classifier__classifier__C=1.0, total=  51.9s\n",
            "[CV] vectoriser__tfidfVectoriser__ngram_range=(1, 1), classifier__classifier__penalty=l2, classifier__classifier__C=1.0 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n",
            "  'stop_words.' % sorted(inconsistent))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  vectoriser__tfidfVectoriser__ngram_range=(1, 1), classifier__classifier__penalty=l2, classifier__classifier__C=1.0, total=  53.1s\n",
            "[CV] vectoriser__tfidfVectoriser__ngram_range=(1, 1), classifier__classifier__penalty=l2, classifier__classifier__C=1.0 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n",
            "  'stop_words.' % sorted(inconsistent))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  vectoriser__tfidfVectoriser__ngram_range=(1, 1), classifier__classifier__penalty=l2, classifier__classifier__C=1.0, total= 1.0min\n",
            "[CV] vectoriser__tfidfVectoriser__ngram_range=(1, 1), classifier__classifier__penalty=l2, classifier__classifier__C=10000.0 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n",
            "  'stop_words.' % sorted(inconsistent))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  vectoriser__tfidfVectoriser__ngram_range=(1, 1), classifier__classifier__penalty=l2, classifier__classifier__C=10000.0, total=  51.3s\n",
            "[CV] vectoriser__tfidfVectoriser__ngram_range=(1, 1), classifier__classifier__penalty=l2, classifier__classifier__C=10000.0 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n",
            "  'stop_words.' % sorted(inconsistent))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  vectoriser__tfidfVectoriser__ngram_range=(1, 1), classifier__classifier__penalty=l2, classifier__classifier__C=10000.0, total=  52.0s\n",
            "[CV] vectoriser__tfidfVectoriser__ngram_range=(1, 1), classifier__classifier__penalty=l2, classifier__classifier__C=10000.0 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n",
            "  'stop_words.' % sorted(inconsistent))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  vectoriser__tfidfVectoriser__ngram_range=(1, 1), classifier__classifier__penalty=l2, classifier__classifier__C=10000.0, total=  51.2s\n",
            "[CV] vectoriser__tfidfVectoriser__ngram_range=(1, 1), classifier__classifier__penalty=l2, classifier__classifier__C=10000.0 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n",
            "  'stop_words.' % sorted(inconsistent))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  vectoriser__tfidfVectoriser__ngram_range=(1, 1), classifier__classifier__penalty=l2, classifier__classifier__C=10000.0, total= 1.0min\n",
            "[CV] vectoriser__tfidfVectoriser__ngram_range=(1, 1), classifier__classifier__penalty=l2, classifier__classifier__C=10000.0 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n",
            "  'stop_words.' % sorted(inconsistent))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  vectoriser__tfidfVectoriser__ngram_range=(1, 1), classifier__classifier__penalty=l2, classifier__classifier__C=10000.0, total=  51.2s\n",
            "[CV] vectoriser__tfidfVectoriser__ngram_range=(1, 1), classifier__classifier__penalty=l1, classifier__classifier__C=10000.0 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n",
            "  'stop_words.' % sorted(inconsistent))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n",
            "  'stop_words.' % sorted(inconsistent))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  vectoriser__tfidfVectoriser__ngram_range=(1, 1), classifier__classifier__penalty=l1, classifier__classifier__C=10000.0, total=  15.6s\n",
            "[CV] vectoriser__tfidfVectoriser__ngram_range=(1, 1), classifier__classifier__penalty=l1, classifier__classifier__C=10000.0 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n",
            "  'stop_words.' % sorted(inconsistent))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  vectoriser__tfidfVectoriser__ngram_range=(1, 1), classifier__classifier__penalty=l1, classifier__classifier__C=10000.0, total=  15.7s\n",
            "[CV] vectoriser__tfidfVectoriser__ngram_range=(1, 1), classifier__classifier__penalty=l1, classifier__classifier__C=10000.0 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n",
            "  'stop_words.' % sorted(inconsistent))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  vectoriser__tfidfVectoriser__ngram_range=(1, 1), classifier__classifier__penalty=l1, classifier__classifier__C=10000.0, total=  15.7s\n",
            "[CV] vectoriser__tfidfVectoriser__ngram_range=(1, 1), classifier__classifier__penalty=l1, classifier__classifier__C=10000.0 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n",
            "  'stop_words.' % sorted(inconsistent))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  vectoriser__tfidfVectoriser__ngram_range=(1, 1), classifier__classifier__penalty=l1, classifier__classifier__C=10000.0, total=  15.7s\n",
            "[CV] vectoriser__tfidfVectoriser__ngram_range=(1, 1), classifier__classifier__penalty=l1, classifier__classifier__C=10000.0 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n",
            "  'stop_words.' % sorted(inconsistent))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  vectoriser__tfidfVectoriser__ngram_range=(1, 1), classifier__classifier__penalty=l1, classifier__classifier__C=10000.0, total=  15.8s\n",
            "[CV] vectoriser__tfidfVectoriser__ngram_range=(1, 2), classifier__classifier__penalty=l1, classifier__classifier__C=1.0 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
            "\n",
            "  FitFailedWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  vectoriser__tfidfVectoriser__ngram_range=(1, 2), classifier__classifier__penalty=l1, classifier__classifier__C=1.0, total= 1.1min\n",
            "[CV] vectoriser__tfidfVectoriser__ngram_range=(1, 2), classifier__classifier__penalty=l1, classifier__classifier__C=1.0 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n",
            "  'stop_words.' % sorted(inconsistent))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
            "\n",
            "  FitFailedWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  vectoriser__tfidfVectoriser__ngram_range=(1, 2), classifier__classifier__penalty=l1, classifier__classifier__C=1.0, total= 1.0min\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n",
            "  'stop_words.' % sorted(inconsistent))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n",
            "  'stop_words.' % sorted(inconsistent))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV] vectoriser__tfidfVectoriser__ngram_range=(1, 2), classifier__classifier__penalty=l1, classifier__classifier__C=1.0 \n",
            "[CV]  vectoriser__tfidfVectoriser__ngram_range=(1, 2), classifier__classifier__penalty=l1, classifier__classifier__C=1.0, total= 1.0min\n",
            "[CV] vectoriser__tfidfVectoriser__ngram_range=(1, 2), classifier__classifier__penalty=l1, classifier__classifier__C=1.0 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
            "\n",
            "  FitFailedWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  vectoriser__tfidfVectoriser__ngram_range=(1, 2), classifier__classifier__penalty=l1, classifier__classifier__C=1.0, total=  58.9s\n",
            "[CV] vectoriser__tfidfVectoriser__ngram_range=(1, 2), classifier__classifier__penalty=l1, classifier__classifier__C=1.0 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n",
            "  'stop_words.' % sorted(inconsistent))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n",
            "  'stop_words.' % sorted(inconsistent))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  vectoriser__tfidfVectoriser__ngram_range=(1, 2), classifier__classifier__penalty=l1, classifier__classifier__C=1.0, total= 1.0min\n",
            "[CV] vectoriser__tfidfVectoriser__ngram_range=(1, 2), classifier__classifier__penalty=l2, classifier__classifier__C=10000.0 \n",
            "[CV]  vectoriser__tfidfVectoriser__ngram_range=(1, 2), classifier__classifier__penalty=l2, classifier__classifier__C=10000.0, total= 6.7min\n",
            "[CV] vectoriser__tfidfVectoriser__ngram_range=(1, 2), classifier__classifier__penalty=l2, classifier__classifier__C=10000.0 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n",
            "  'stop_words.' % sorted(inconsistent))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  vectoriser__tfidfVectoriser__ngram_range=(1, 2), classifier__classifier__penalty=l2, classifier__classifier__C=10000.0, total= 6.7min\n",
            "[CV] vectoriser__tfidfVectoriser__ngram_range=(1, 2), classifier__classifier__penalty=l2, classifier__classifier__C=10000.0 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n",
            "  'stop_words.' % sorted(inconsistent))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  vectoriser__tfidfVectoriser__ngram_range=(1, 2), classifier__classifier__penalty=l2, classifier__classifier__C=10000.0, total= 6.8min\n",
            "[CV] vectoriser__tfidfVectoriser__ngram_range=(1, 2), classifier__classifier__penalty=l2, classifier__classifier__C=10000.0 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n",
            "  'stop_words.' % sorted(inconsistent))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  vectoriser__tfidfVectoriser__ngram_range=(1, 2), classifier__classifier__penalty=l2, classifier__classifier__C=10000.0, total= 6.7min\n",
            "[CV] vectoriser__tfidfVectoriser__ngram_range=(1, 2), classifier__classifier__penalty=l2, classifier__classifier__C=10000.0 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n",
            "  'stop_words.' % sorted(inconsistent))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  vectoriser__tfidfVectoriser__ngram_range=(1, 2), classifier__classifier__penalty=l2, classifier__classifier__C=10000.0, total= 6.7min\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done  50 out of  50 | elapsed: 95.1min finished\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n",
            "  'stop_words.' % sorted(inconsistent))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>rank</th>\n",
              "      <th>params</th>\n",
              "      <th>cv score (mean)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>{'vectoriser__tfidfVectoriser__ngram_range': (1, 2), 'classifier__classifier__penalty': 'l2', 'classifier__classifier__C': 1.0}</td>\n",
              "      <td>0.800092</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>2</td>\n",
              "      <td>{'vectoriser__tfidfVectoriser__ngram_range': (1, 2), 'classifier__classifier__penalty': 'l2', 'classifier__classifier__C': 10000.0}</td>\n",
              "      <td>0.789472</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>3</td>\n",
              "      <td>{'vectoriser__tfidfVectoriser__ngram_range': (1, 1), 'classifier__classifier__penalty': 'l2', 'classifier__classifier__C': 1.0}</td>\n",
              "      <td>0.787372</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>4</td>\n",
              "      <td>{'vectoriser__tfidfVectoriser__ngram_range': (1, 1), 'classifier__classifier__penalty': 'l2', 'classifier__classifier__C': 100.0}</td>\n",
              "      <td>0.780099</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>5</td>\n",
              "      <td>{'vectoriser__tfidfVectoriser__ngram_range': (1, 1), 'classifier__classifier__penalty': 'l2', 'classifier__classifier__C': 10000.0}</td>\n",
              "      <td>0.778698</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>6</td>\n",
              "      <td>{'vectoriser__tfidfVectoriser__ngram_range': (1, 2), 'classifier__classifier__penalty': 'l1', 'classifier__classifier__C': 100.0}</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>7</td>\n",
              "      <td>{'vectoriser__tfidfVectoriser__ngram_range': (1, 1), 'classifier__classifier__penalty': 'l1', 'classifier__classifier__C': 1.0}</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>8</td>\n",
              "      <td>{'vectoriser__tfidfVectoriser__ngram_range': (1, 1), 'classifier__classifier__penalty': 'l1', 'classifier__classifier__C': 100.0}</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>9</td>\n",
              "      <td>{'vectoriser__tfidfVectoriser__ngram_range': (1, 1), 'classifier__classifier__penalty': 'l1', 'classifier__classifier__C': 10000.0}</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>10</td>\n",
              "      <td>{'vectoriser__tfidfVectoriser__ngram_range': (1, 2), 'classifier__classifier__penalty': 'l1', 'classifier__classifier__C': 1.0}</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "h1FMt2Evc9ag"
      },
      "source": [
        "\n",
        "\n",
        " Decision Tree - Fine Tune"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ObhcwhKifZZU",
        "colab": {}
      },
      "source": [
        "from sklearn.tree import DecisionTreeClassifier"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "wdBW7C0HdBfc",
        "colab": {}
      },
      "source": [
        "param_grid = {\"classifier__classifier__min_samples_leaf\" : [1, 5, 10, 20, 50, 100],\n",
        "               'vectoriser__tfidfVectoriser__ngram_range': [(1,1),(1,2)]}\n",
        "\n",
        "\n",
        "decision_tree = Pipeline([\n",
        "    ('classifier', DecisionTreeClassifier(max_depth=25)),\n",
        "])\n",
        "\n",
        "pipeline = Pipeline([\n",
        "    ('vectoriser', tfidf_vectoriser),\n",
        "    ('classifier', decision_tree)\n",
        "])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "1lrko4Bjxf3I",
        "colab_type": "code",
        "outputId": "d0312eae-8599-4744-c779-fad31ad513ee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 585
        }
      },
      "source": [
        "gs_mnb = GridSearchCV(pipeline, param_grid, cv=5, verbose=2, n_jobs=-1)\n",
        "results = train_predict_all(gs_mnb)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
            "/usr/local/lib/python3.6/dist-packages/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  \"timeout or by a memory leak.\", UserWarning\n",
            "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed: 169.5min\n",
            "[Parallel(n_jobs=-1)]: Done  60 out of  60 | elapsed: 250.2min finished\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n",
            "  'stop_words.' % sorted(inconsistent))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>rank</th>\n",
              "      <th>params</th>\n",
              "      <th>cv score (mean)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>1</td>\n",
              "      <td>{'classifier__classifier__min_samples_leaf': 50, 'vectoriser__tfidfVectoriser__ngram_range': (1, 2)}</td>\n",
              "      <td>0.634169</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>2</td>\n",
              "      <td>{'classifier__classifier__min_samples_leaf': 20, 'vectoriser__tfidfVectoriser__ngram_range': (1, 2)}</td>\n",
              "      <td>0.633832</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>3</td>\n",
              "      <td>{'classifier__classifier__min_samples_leaf': 100, 'vectoriser__tfidfVectoriser__ngram_range': (1, 2)}</td>\n",
              "      <td>0.632276</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>4</td>\n",
              "      <td>{'classifier__classifier__min_samples_leaf': 20, 'vectoriser__tfidfVectoriser__ngram_range': (1, 1)}</td>\n",
              "      <td>0.632143</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>5</td>\n",
              "      <td>{'classifier__classifier__min_samples_leaf': 50, 'vectoriser__tfidfVectoriser__ngram_range': (1, 1)}</td>\n",
              "      <td>0.632033</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>6</td>\n",
              "      <td>{'classifier__classifier__min_samples_leaf': 1, 'vectoriser__tfidfVectoriser__ngram_range': (1, 2)}</td>\n",
              "      <td>0.631392</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>7</td>\n",
              "      <td>{'classifier__classifier__min_samples_leaf': 100, 'vectoriser__tfidfVectoriser__ngram_range': (1, 1)}</td>\n",
              "      <td>0.630971</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>8</td>\n",
              "      <td>{'classifier__classifier__min_samples_leaf': 10, 'vectoriser__tfidfVectoriser__ngram_range': (1, 2)}</td>\n",
              "      <td>0.630784</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>9</td>\n",
              "      <td>{'classifier__classifier__min_samples_leaf': 1, 'vectoriser__tfidfVectoriser__ngram_range': (1, 1)}</td>\n",
              "      <td>0.629336</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>10</td>\n",
              "      <td>{'classifier__classifier__min_samples_leaf': 10, 'vectoriser__tfidfVectoriser__ngram_range': (1, 1)}</td>\n",
              "      <td>0.628461</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>11</td>\n",
              "      <td>{'classifier__classifier__min_samples_leaf': 5, 'vectoriser__tfidfVectoriser__ngram_range': (1, 2)}</td>\n",
              "      <td>0.627256</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>12</td>\n",
              "      <td>{'classifier__classifier__min_samples_leaf': 5, 'vectoriser__tfidfVectoriser__ngram_range': (1, 1)}</td>\n",
              "      <td>0.626495</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "1yA0yrU-u9Hg",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "### Bagging Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_m6mNSsfu9Hg",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "Random Forest - Fine Tune"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "R_1d7-Lbu9Hg",
        "pycharm": {
          "name": "#%%\n"
        },
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "# Number of trees in random forest\n",
        "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
        "# Number of features to consider at every split\n",
        "max_features = ['auto', 'sqrt']\n",
        "# Minimum number of samples required to split a node\n",
        "min_samples_split = [2, 5, 10]\n",
        "# Minimum number of samples required at each leaf node\n",
        "min_samples_leaf = [1, 2, 4]\n",
        "# Method of selecting samples for training each tree\n",
        "bootstrap = [True, False]# Create the random grid\n",
        "param_grid = {'classifier__classifier__n_estimators': n_estimators,\n",
        "               'classifier__classifier__max_features': max_features,\n",
        "               'classifier__classifier__min_samples_split': min_samples_split,\n",
        "               'classifier__classifier__min_samples_leaf': min_samples_leaf,\n",
        "               'classifier__classifier__bootstrap': bootstrap}\n",
        "\n",
        "random_forest = Pipeline([\n",
        "    ('classifier', RandomForestClassifier(max_depth=20, n_jobs=-1)),\n",
        "])\n",
        "\n",
        "pipeline = Pipeline([\n",
        "    ('vectoriser', tfidf_vectoriser),\n",
        "    ('classifier', random_forest)\n",
        "])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "C_-o8KD7u9Hg",
        "pycharm": {
          "name": "#%%\n"
        },
        "outputId": "1cefaae9-1961-4b08-f29a-023453ece6f0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "gs_mnb = RandomizedSearchCV(pipeline, param_grid, cv=5, verbose=2)\n",
        "results = train_predict_all(gs_mnb)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
            "[CV] classifier__classifier__n_estimators=1800, classifier__classifier__min_samples_split=2, classifier__classifier__min_samples_leaf=2, classifier__classifier__max_features=sqrt, classifier__classifier__bootstrap=False \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n",
            "  'stop_words.' % sorted(inconsistent))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  classifier__classifier__n_estimators=1800, classifier__classifier__min_samples_split=2, classifier__classifier__min_samples_leaf=2, classifier__classifier__max_features=sqrt, classifier__classifier__bootstrap=False, total= 4.0min\n",
            "[CV] classifier__classifier__n_estimators=1800, classifier__classifier__min_samples_split=2, classifier__classifier__min_samples_leaf=2, classifier__classifier__max_features=sqrt, classifier__classifier__bootstrap=False \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  4.0min remaining:    0.0s\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n",
            "  'stop_words.' % sorted(inconsistent))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  classifier__classifier__n_estimators=1800, classifier__classifier__min_samples_split=2, classifier__classifier__min_samples_leaf=2, classifier__classifier__max_features=sqrt, classifier__classifier__bootstrap=False, total= 4.0min\n",
            "[CV] classifier__classifier__n_estimators=1800, classifier__classifier__min_samples_split=2, classifier__classifier__min_samples_leaf=2, classifier__classifier__max_features=sqrt, classifier__classifier__bootstrap=False \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n",
            "  'stop_words.' % sorted(inconsistent))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  classifier__classifier__n_estimators=1800, classifier__classifier__min_samples_split=2, classifier__classifier__min_samples_leaf=2, classifier__classifier__max_features=sqrt, classifier__classifier__bootstrap=False, total= 4.0min\n",
            "[CV] classifier__classifier__n_estimators=1800, classifier__classifier__min_samples_split=2, classifier__classifier__min_samples_leaf=2, classifier__classifier__max_features=sqrt, classifier__classifier__bootstrap=False \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n",
            "  'stop_words.' % sorted(inconsistent))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  classifier__classifier__n_estimators=1800, classifier__classifier__min_samples_split=2, classifier__classifier__min_samples_leaf=2, classifier__classifier__max_features=sqrt, classifier__classifier__bootstrap=False, total= 4.0min\n",
            "[CV] classifier__classifier__n_estimators=1800, classifier__classifier__min_samples_split=2, classifier__classifier__min_samples_leaf=2, classifier__classifier__max_features=sqrt, classifier__classifier__bootstrap=False \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n",
            "  'stop_words.' % sorted(inconsistent))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  classifier__classifier__n_estimators=1800, classifier__classifier__min_samples_split=2, classifier__classifier__min_samples_leaf=2, classifier__classifier__max_features=sqrt, classifier__classifier__bootstrap=False, total= 4.0min\n",
            "[CV] classifier__classifier__n_estimators=400, classifier__classifier__min_samples_split=2, classifier__classifier__min_samples_leaf=2, classifier__classifier__max_features=auto, classifier__classifier__bootstrap=True \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n",
            "  'stop_words.' % sorted(inconsistent))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  classifier__classifier__n_estimators=400, classifier__classifier__min_samples_split=2, classifier__classifier__min_samples_leaf=2, classifier__classifier__max_features=auto, classifier__classifier__bootstrap=True, total= 1.0min\n",
            "[CV] classifier__classifier__n_estimators=400, classifier__classifier__min_samples_split=2, classifier__classifier__min_samples_leaf=2, classifier__classifier__max_features=auto, classifier__classifier__bootstrap=True \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n",
            "  'stop_words.' % sorted(inconsistent))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  classifier__classifier__n_estimators=400, classifier__classifier__min_samples_split=2, classifier__classifier__min_samples_leaf=2, classifier__classifier__max_features=auto, classifier__classifier__bootstrap=True, total= 1.0min\n",
            "[CV] classifier__classifier__n_estimators=400, classifier__classifier__min_samples_split=2, classifier__classifier__min_samples_leaf=2, classifier__classifier__max_features=auto, classifier__classifier__bootstrap=True \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n",
            "  'stop_words.' % sorted(inconsistent))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  classifier__classifier__n_estimators=400, classifier__classifier__min_samples_split=2, classifier__classifier__min_samples_leaf=2, classifier__classifier__max_features=auto, classifier__classifier__bootstrap=True, total= 1.0min\n",
            "[CV] classifier__classifier__n_estimators=400, classifier__classifier__min_samples_split=2, classifier__classifier__min_samples_leaf=2, classifier__classifier__max_features=auto, classifier__classifier__bootstrap=True \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n",
            "  'stop_words.' % sorted(inconsistent))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  classifier__classifier__n_estimators=400, classifier__classifier__min_samples_split=2, classifier__classifier__min_samples_leaf=2, classifier__classifier__max_features=auto, classifier__classifier__bootstrap=True, total= 1.0min\n",
            "[CV] classifier__classifier__n_estimators=400, classifier__classifier__min_samples_split=2, classifier__classifier__min_samples_leaf=2, classifier__classifier__max_features=auto, classifier__classifier__bootstrap=True \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n",
            "  'stop_words.' % sorted(inconsistent))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  classifier__classifier__n_estimators=400, classifier__classifier__min_samples_split=2, classifier__classifier__min_samples_leaf=2, classifier__classifier__max_features=auto, classifier__classifier__bootstrap=True, total= 1.0min\n",
            "[CV] classifier__classifier__n_estimators=1400, classifier__classifier__min_samples_split=5, classifier__classifier__min_samples_leaf=1, classifier__classifier__max_features=sqrt, classifier__classifier__bootstrap=True \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n",
            "  'stop_words.' % sorted(inconsistent))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  classifier__classifier__n_estimators=1400, classifier__classifier__min_samples_split=5, classifier__classifier__min_samples_leaf=1, classifier__classifier__max_features=sqrt, classifier__classifier__bootstrap=True, total= 3.2min\n",
            "[CV] classifier__classifier__n_estimators=1400, classifier__classifier__min_samples_split=5, classifier__classifier__min_samples_leaf=1, classifier__classifier__max_features=sqrt, classifier__classifier__bootstrap=True \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n",
            "  'stop_words.' % sorted(inconsistent))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  classifier__classifier__n_estimators=1400, classifier__classifier__min_samples_split=5, classifier__classifier__min_samples_leaf=1, classifier__classifier__max_features=sqrt, classifier__classifier__bootstrap=True, total= 3.2min\n",
            "[CV] classifier__classifier__n_estimators=1400, classifier__classifier__min_samples_split=5, classifier__classifier__min_samples_leaf=1, classifier__classifier__max_features=sqrt, classifier__classifier__bootstrap=True \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n",
            "  'stop_words.' % sorted(inconsistent))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  classifier__classifier__n_estimators=1400, classifier__classifier__min_samples_split=5, classifier__classifier__min_samples_leaf=1, classifier__classifier__max_features=sqrt, classifier__classifier__bootstrap=True, total= 3.2min\n",
            "[CV] classifier__classifier__n_estimators=1400, classifier__classifier__min_samples_split=5, classifier__classifier__min_samples_leaf=1, classifier__classifier__max_features=sqrt, classifier__classifier__bootstrap=True \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n",
            "  'stop_words.' % sorted(inconsistent))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  classifier__classifier__n_estimators=1400, classifier__classifier__min_samples_split=5, classifier__classifier__min_samples_leaf=1, classifier__classifier__max_features=sqrt, classifier__classifier__bootstrap=True, total= 3.2min\n",
            "[CV] classifier__classifier__n_estimators=1400, classifier__classifier__min_samples_split=5, classifier__classifier__min_samples_leaf=1, classifier__classifier__max_features=sqrt, classifier__classifier__bootstrap=True \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n",
            "  'stop_words.' % sorted(inconsistent))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  classifier__classifier__n_estimators=1400, classifier__classifier__min_samples_split=5, classifier__classifier__min_samples_leaf=1, classifier__classifier__max_features=sqrt, classifier__classifier__bootstrap=True, total= 3.2min\n",
            "[CV] classifier__classifier__n_estimators=2000, classifier__classifier__min_samples_split=2, classifier__classifier__min_samples_leaf=4, classifier__classifier__max_features=auto, classifier__classifier__bootstrap=False \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n",
            "  'stop_words.' % sorted(inconsistent))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  classifier__classifier__n_estimators=2000, classifier__classifier__min_samples_split=2, classifier__classifier__min_samples_leaf=4, classifier__classifier__max_features=auto, classifier__classifier__bootstrap=False, total= 3.9min\n",
            "[CV] classifier__classifier__n_estimators=2000, classifier__classifier__min_samples_split=2, classifier__classifier__min_samples_leaf=4, classifier__classifier__max_features=auto, classifier__classifier__bootstrap=False \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n",
            "  'stop_words.' % sorted(inconsistent))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  classifier__classifier__n_estimators=2000, classifier__classifier__min_samples_split=2, classifier__classifier__min_samples_leaf=4, classifier__classifier__max_features=auto, classifier__classifier__bootstrap=False, total= 3.9min\n",
            "[CV] classifier__classifier__n_estimators=2000, classifier__classifier__min_samples_split=2, classifier__classifier__min_samples_leaf=4, classifier__classifier__max_features=auto, classifier__classifier__bootstrap=False \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n",
            "  'stop_words.' % sorted(inconsistent))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  classifier__classifier__n_estimators=2000, classifier__classifier__min_samples_split=2, classifier__classifier__min_samples_leaf=4, classifier__classifier__max_features=auto, classifier__classifier__bootstrap=False, total= 3.9min\n",
            "[CV] classifier__classifier__n_estimators=2000, classifier__classifier__min_samples_split=2, classifier__classifier__min_samples_leaf=4, classifier__classifier__max_features=auto, classifier__classifier__bootstrap=False \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n",
            "  'stop_words.' % sorted(inconsistent))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  classifier__classifier__n_estimators=2000, classifier__classifier__min_samples_split=2, classifier__classifier__min_samples_leaf=4, classifier__classifier__max_features=auto, classifier__classifier__bootstrap=False, total= 3.9min\n",
            "[CV] classifier__classifier__n_estimators=2000, classifier__classifier__min_samples_split=2, classifier__classifier__min_samples_leaf=4, classifier__classifier__max_features=auto, classifier__classifier__bootstrap=False \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n",
            "  'stop_words.' % sorted(inconsistent))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  classifier__classifier__n_estimators=2000, classifier__classifier__min_samples_split=2, classifier__classifier__min_samples_leaf=4, classifier__classifier__max_features=auto, classifier__classifier__bootstrap=False, total= 3.9min\n",
            "[CV] classifier__classifier__n_estimators=1200, classifier__classifier__min_samples_split=5, classifier__classifier__min_samples_leaf=1, classifier__classifier__max_features=auto, classifier__classifier__bootstrap=True \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n",
            "  'stop_words.' % sorted(inconsistent))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  classifier__classifier__n_estimators=1200, classifier__classifier__min_samples_split=5, classifier__classifier__min_samples_leaf=1, classifier__classifier__max_features=auto, classifier__classifier__bootstrap=True, total= 2.8min\n",
            "[CV] classifier__classifier__n_estimators=1200, classifier__classifier__min_samples_split=5, classifier__classifier__min_samples_leaf=1, classifier__classifier__max_features=auto, classifier__classifier__bootstrap=True \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n",
            "  'stop_words.' % sorted(inconsistent))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  classifier__classifier__n_estimators=1200, classifier__classifier__min_samples_split=5, classifier__classifier__min_samples_leaf=1, classifier__classifier__max_features=auto, classifier__classifier__bootstrap=True, total= 2.8min\n",
            "[CV] classifier__classifier__n_estimators=1200, classifier__classifier__min_samples_split=5, classifier__classifier__min_samples_leaf=1, classifier__classifier__max_features=auto, classifier__classifier__bootstrap=True \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n",
            "  'stop_words.' % sorted(inconsistent))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  classifier__classifier__n_estimators=1200, classifier__classifier__min_samples_split=5, classifier__classifier__min_samples_leaf=1, classifier__classifier__max_features=auto, classifier__classifier__bootstrap=True, total= 2.8min\n",
            "[CV] classifier__classifier__n_estimators=1200, classifier__classifier__min_samples_split=5, classifier__classifier__min_samples_leaf=1, classifier__classifier__max_features=auto, classifier__classifier__bootstrap=True \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n",
            "  'stop_words.' % sorted(inconsistent))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  classifier__classifier__n_estimators=1200, classifier__classifier__min_samples_split=5, classifier__classifier__min_samples_leaf=1, classifier__classifier__max_features=auto, classifier__classifier__bootstrap=True, total= 2.8min\n",
            "[CV] classifier__classifier__n_estimators=1200, classifier__classifier__min_samples_split=5, classifier__classifier__min_samples_leaf=1, classifier__classifier__max_features=auto, classifier__classifier__bootstrap=True \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n",
            "  'stop_words.' % sorted(inconsistent))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  classifier__classifier__n_estimators=1200, classifier__classifier__min_samples_split=5, classifier__classifier__min_samples_leaf=1, classifier__classifier__max_features=auto, classifier__classifier__bootstrap=True, total= 2.8min\n",
            "[CV] classifier__classifier__n_estimators=600, classifier__classifier__min_samples_split=10, classifier__classifier__min_samples_leaf=1, classifier__classifier__max_features=auto, classifier__classifier__bootstrap=False \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n",
            "  'stop_words.' % sorted(inconsistent))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  classifier__classifier__n_estimators=600, classifier__classifier__min_samples_split=10, classifier__classifier__min_samples_leaf=1, classifier__classifier__max_features=auto, classifier__classifier__bootstrap=False, total= 1.7min\n",
            "[CV] classifier__classifier__n_estimators=600, classifier__classifier__min_samples_split=10, classifier__classifier__min_samples_leaf=1, classifier__classifier__max_features=auto, classifier__classifier__bootstrap=False \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n",
            "  'stop_words.' % sorted(inconsistent))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  classifier__classifier__n_estimators=600, classifier__classifier__min_samples_split=10, classifier__classifier__min_samples_leaf=1, classifier__classifier__max_features=auto, classifier__classifier__bootstrap=False, total= 1.7min\n",
            "[CV] classifier__classifier__n_estimators=600, classifier__classifier__min_samples_split=10, classifier__classifier__min_samples_leaf=1, classifier__classifier__max_features=auto, classifier__classifier__bootstrap=False \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n",
            "  'stop_words.' % sorted(inconsistent))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  classifier__classifier__n_estimators=600, classifier__classifier__min_samples_split=10, classifier__classifier__min_samples_leaf=1, classifier__classifier__max_features=auto, classifier__classifier__bootstrap=False, total= 1.7min\n",
            "[CV] classifier__classifier__n_estimators=600, classifier__classifier__min_samples_split=10, classifier__classifier__min_samples_leaf=1, classifier__classifier__max_features=auto, classifier__classifier__bootstrap=False \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n",
            "  'stop_words.' % sorted(inconsistent))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  classifier__classifier__n_estimators=600, classifier__classifier__min_samples_split=10, classifier__classifier__min_samples_leaf=1, classifier__classifier__max_features=auto, classifier__classifier__bootstrap=False, total= 1.7min\n",
            "[CV] classifier__classifier__n_estimators=600, classifier__classifier__min_samples_split=10, classifier__classifier__min_samples_leaf=1, classifier__classifier__max_features=auto, classifier__classifier__bootstrap=False \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n",
            "  'stop_words.' % sorted(inconsistent))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  classifier__classifier__n_estimators=600, classifier__classifier__min_samples_split=10, classifier__classifier__min_samples_leaf=1, classifier__classifier__max_features=auto, classifier__classifier__bootstrap=False, total= 1.7min\n",
            "[CV] classifier__classifier__n_estimators=1600, classifier__classifier__min_samples_split=10, classifier__classifier__min_samples_leaf=4, classifier__classifier__max_features=auto, classifier__classifier__bootstrap=True \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n",
            "  'stop_words.' % sorted(inconsistent))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  classifier__classifier__n_estimators=1600, classifier__classifier__min_samples_split=10, classifier__classifier__min_samples_leaf=4, classifier__classifier__max_features=auto, classifier__classifier__bootstrap=True, total= 2.7min\n",
            "[CV] classifier__classifier__n_estimators=1600, classifier__classifier__min_samples_split=10, classifier__classifier__min_samples_leaf=4, classifier__classifier__max_features=auto, classifier__classifier__bootstrap=True \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n",
            "  'stop_words.' % sorted(inconsistent))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  classifier__classifier__n_estimators=1600, classifier__classifier__min_samples_split=10, classifier__classifier__min_samples_leaf=4, classifier__classifier__max_features=auto, classifier__classifier__bootstrap=True, total= 2.7min\n",
            "[CV] classifier__classifier__n_estimators=1600, classifier__classifier__min_samples_split=10, classifier__classifier__min_samples_leaf=4, classifier__classifier__max_features=auto, classifier__classifier__bootstrap=True \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n",
            "  'stop_words.' % sorted(inconsistent))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  classifier__classifier__n_estimators=1600, classifier__classifier__min_samples_split=10, classifier__classifier__min_samples_leaf=4, classifier__classifier__max_features=auto, classifier__classifier__bootstrap=True, total= 2.7min\n",
            "[CV] classifier__classifier__n_estimators=1600, classifier__classifier__min_samples_split=10, classifier__classifier__min_samples_leaf=4, classifier__classifier__max_features=auto, classifier__classifier__bootstrap=True \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n",
            "  'stop_words.' % sorted(inconsistent))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  classifier__classifier__n_estimators=1600, classifier__classifier__min_samples_split=10, classifier__classifier__min_samples_leaf=4, classifier__classifier__max_features=auto, classifier__classifier__bootstrap=True, total= 2.7min\n",
            "[CV] classifier__classifier__n_estimators=1600, classifier__classifier__min_samples_split=10, classifier__classifier__min_samples_leaf=4, classifier__classifier__max_features=auto, classifier__classifier__bootstrap=True \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n",
            "  'stop_words.' % sorted(inconsistent))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  classifier__classifier__n_estimators=1600, classifier__classifier__min_samples_split=10, classifier__classifier__min_samples_leaf=4, classifier__classifier__max_features=auto, classifier__classifier__bootstrap=True, total= 2.7min\n",
            "[CV] classifier__classifier__n_estimators=1600, classifier__classifier__min_samples_split=5, classifier__classifier__min_samples_leaf=2, classifier__classifier__max_features=auto, classifier__classifier__bootstrap=False \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n",
            "  'stop_words.' % sorted(inconsistent))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  classifier__classifier__n_estimators=1600, classifier__classifier__min_samples_split=5, classifier__classifier__min_samples_leaf=2, classifier__classifier__max_features=auto, classifier__classifier__bootstrap=False, total= 3.6min\n",
            "[CV] classifier__classifier__n_estimators=1600, classifier__classifier__min_samples_split=5, classifier__classifier__min_samples_leaf=2, classifier__classifier__max_features=auto, classifier__classifier__bootstrap=False \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n",
            "  'stop_words.' % sorted(inconsistent))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  classifier__classifier__n_estimators=1600, classifier__classifier__min_samples_split=5, classifier__classifier__min_samples_leaf=2, classifier__classifier__max_features=auto, classifier__classifier__bootstrap=False, total= 3.6min\n",
            "[CV] classifier__classifier__n_estimators=1600, classifier__classifier__min_samples_split=5, classifier__classifier__min_samples_leaf=2, classifier__classifier__max_features=auto, classifier__classifier__bootstrap=False \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n",
            "  'stop_words.' % sorted(inconsistent))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  classifier__classifier__n_estimators=1600, classifier__classifier__min_samples_split=5, classifier__classifier__min_samples_leaf=2, classifier__classifier__max_features=auto, classifier__classifier__bootstrap=False, total= 3.7min\n",
            "[CV] classifier__classifier__n_estimators=1600, classifier__classifier__min_samples_split=5, classifier__classifier__min_samples_leaf=2, classifier__classifier__max_features=auto, classifier__classifier__bootstrap=False \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n",
            "  'stop_words.' % sorted(inconsistent))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  classifier__classifier__n_estimators=1600, classifier__classifier__min_samples_split=5, classifier__classifier__min_samples_leaf=2, classifier__classifier__max_features=auto, classifier__classifier__bootstrap=False, total= 3.6min\n",
            "[CV] classifier__classifier__n_estimators=1600, classifier__classifier__min_samples_split=5, classifier__classifier__min_samples_leaf=2, classifier__classifier__max_features=auto, classifier__classifier__bootstrap=False \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n",
            "  'stop_words.' % sorted(inconsistent))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  classifier__classifier__n_estimators=1600, classifier__classifier__min_samples_split=5, classifier__classifier__min_samples_leaf=2, classifier__classifier__max_features=auto, classifier__classifier__bootstrap=False, total= 3.7min\n",
            "[CV] classifier__classifier__n_estimators=2000, classifier__classifier__min_samples_split=2, classifier__classifier__min_samples_leaf=2, classifier__classifier__max_features=auto, classifier__classifier__bootstrap=False \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n",
            "  'stop_words.' % sorted(inconsistent))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  classifier__classifier__n_estimators=2000, classifier__classifier__min_samples_split=2, classifier__classifier__min_samples_leaf=2, classifier__classifier__max_features=auto, classifier__classifier__bootstrap=False, total= 4.6min\n",
            "[CV] classifier__classifier__n_estimators=2000, classifier__classifier__min_samples_split=2, classifier__classifier__min_samples_leaf=2, classifier__classifier__max_features=auto, classifier__classifier__bootstrap=False \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n",
            "  'stop_words.' % sorted(inconsistent))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  classifier__classifier__n_estimators=2000, classifier__classifier__min_samples_split=2, classifier__classifier__min_samples_leaf=2, classifier__classifier__max_features=auto, classifier__classifier__bootstrap=False, total= 4.5min\n",
            "[CV] classifier__classifier__n_estimators=2000, classifier__classifier__min_samples_split=2, classifier__classifier__min_samples_leaf=2, classifier__classifier__max_features=auto, classifier__classifier__bootstrap=False \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n",
            "  'stop_words.' % sorted(inconsistent))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  classifier__classifier__n_estimators=2000, classifier__classifier__min_samples_split=2, classifier__classifier__min_samples_leaf=2, classifier__classifier__max_features=auto, classifier__classifier__bootstrap=False, total= 4.5min\n",
            "[CV] classifier__classifier__n_estimators=2000, classifier__classifier__min_samples_split=2, classifier__classifier__min_samples_leaf=2, classifier__classifier__max_features=auto, classifier__classifier__bootstrap=False \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n",
            "  'stop_words.' % sorted(inconsistent))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  classifier__classifier__n_estimators=2000, classifier__classifier__min_samples_split=2, classifier__classifier__min_samples_leaf=2, classifier__classifier__max_features=auto, classifier__classifier__bootstrap=False, total= 4.4min\n",
            "[CV] classifier__classifier__n_estimators=2000, classifier__classifier__min_samples_split=2, classifier__classifier__min_samples_leaf=2, classifier__classifier__max_features=auto, classifier__classifier__bootstrap=False \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n",
            "  'stop_words.' % sorted(inconsistent))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  classifier__classifier__n_estimators=2000, classifier__classifier__min_samples_split=2, classifier__classifier__min_samples_leaf=2, classifier__classifier__max_features=auto, classifier__classifier__bootstrap=False, total= 4.5min\n",
            "[CV] classifier__classifier__n_estimators=400, classifier__classifier__min_samples_split=10, classifier__classifier__min_samples_leaf=1, classifier__classifier__max_features=auto, classifier__classifier__bootstrap=True \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n",
            "  'stop_words.' % sorted(inconsistent))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  classifier__classifier__n_estimators=400, classifier__classifier__min_samples_split=10, classifier__classifier__min_samples_leaf=1, classifier__classifier__max_features=auto, classifier__classifier__bootstrap=True, total= 1.1min\n",
            "[CV] classifier__classifier__n_estimators=400, classifier__classifier__min_samples_split=10, classifier__classifier__min_samples_leaf=1, classifier__classifier__max_features=auto, classifier__classifier__bootstrap=True \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n",
            "  'stop_words.' % sorted(inconsistent))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  classifier__classifier__n_estimators=400, classifier__classifier__min_samples_split=10, classifier__classifier__min_samples_leaf=1, classifier__classifier__max_features=auto, classifier__classifier__bootstrap=True, total= 1.1min\n",
            "[CV] classifier__classifier__n_estimators=400, classifier__classifier__min_samples_split=10, classifier__classifier__min_samples_leaf=1, classifier__classifier__max_features=auto, classifier__classifier__bootstrap=True \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n",
            "  'stop_words.' % sorted(inconsistent))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  classifier__classifier__n_estimators=400, classifier__classifier__min_samples_split=10, classifier__classifier__min_samples_leaf=1, classifier__classifier__max_features=auto, classifier__classifier__bootstrap=True, total= 1.1min\n",
            "[CV] classifier__classifier__n_estimators=400, classifier__classifier__min_samples_split=10, classifier__classifier__min_samples_leaf=1, classifier__classifier__max_features=auto, classifier__classifier__bootstrap=True \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n",
            "  'stop_words.' % sorted(inconsistent))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  classifier__classifier__n_estimators=400, classifier__classifier__min_samples_split=10, classifier__classifier__min_samples_leaf=1, classifier__classifier__max_features=auto, classifier__classifier__bootstrap=True, total= 1.1min\n",
            "[CV] classifier__classifier__n_estimators=400, classifier__classifier__min_samples_split=10, classifier__classifier__min_samples_leaf=1, classifier__classifier__max_features=auto, classifier__classifier__bootstrap=True \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n",
            "  'stop_words.' % sorted(inconsistent))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  classifier__classifier__n_estimators=400, classifier__classifier__min_samples_split=10, classifier__classifier__min_samples_leaf=1, classifier__classifier__max_features=auto, classifier__classifier__bootstrap=True, total= 1.1min\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done  50 out of  50 | elapsed: 143.5min finished\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n",
            "  'stop_words.' % sorted(inconsistent))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>rank</th>\n",
              "      <th>params</th>\n",
              "      <th>cv score (mean)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>{'classifier__classifier__n_estimators': 1200, 'classifier__classifier__min_samples_split': 5, 'classifier__classifier__min_samples_leaf': 1, 'classifier__classifier__max_features': 'auto', 'classifier__classifier__bootstrap': True}</td>\n",
              "      <td>0.727314</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>{'classifier__classifier__n_estimators': 1400, 'classifier__classifier__min_samples_split': 5, 'classifier__classifier__min_samples_leaf': 1, 'classifier__classifier__max_features': 'sqrt', 'classifier__classifier__bootstrap': True}</td>\n",
              "      <td>0.727137</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>3</td>\n",
              "      <td>{'classifier__classifier__n_estimators': 1600, 'classifier__classifier__min_samples_split': 5, 'classifier__classifier__min_samples_leaf': 2, 'classifier__classifier__max_features': 'auto', 'classifier__classifier__bootstrap': False}</td>\n",
              "      <td>0.726943</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>{'classifier__classifier__n_estimators': 2000, 'classifier__classifier__min_samples_split': 2, 'classifier__classifier__min_samples_leaf': 4, 'classifier__classifier__max_features': 'auto', 'classifier__classifier__bootstrap': False}</td>\n",
              "      <td>0.726890</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>5</td>\n",
              "      <td>{'classifier__classifier__n_estimators': 1600, 'classifier__classifier__min_samples_split': 10, 'classifier__classifier__min_samples_leaf': 4, 'classifier__classifier__max_features': 'auto', 'classifier__classifier__bootstrap': True}</td>\n",
              "      <td>0.726790</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>6</td>\n",
              "      <td>{'classifier__classifier__n_estimators': 2000, 'classifier__classifier__min_samples_split': 2, 'classifier__classifier__min_samples_leaf': 2, 'classifier__classifier__max_features': 'auto', 'classifier__classifier__bootstrap': False}</td>\n",
              "      <td>0.726486</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>7</td>\n",
              "      <td>{'classifier__classifier__n_estimators': 1800, 'classifier__classifier__min_samples_split': 2, 'classifier__classifier__min_samples_leaf': 2, 'classifier__classifier__max_features': 'sqrt', 'classifier__classifier__bootstrap': False}</td>\n",
              "      <td>0.726362</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>8</td>\n",
              "      <td>{'classifier__classifier__n_estimators': 600, 'classifier__classifier__min_samples_split': 10, 'classifier__classifier__min_samples_leaf': 1, 'classifier__classifier__max_features': 'auto', 'classifier__classifier__bootstrap': False}</td>\n",
              "      <td>0.725364</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>9</td>\n",
              "      <td>{'classifier__classifier__n_estimators': 400, 'classifier__classifier__min_samples_split': 10, 'classifier__classifier__min_samples_leaf': 1, 'classifier__classifier__max_features': 'auto', 'classifier__classifier__bootstrap': True}</td>\n",
              "      <td>0.723939</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>10</td>\n",
              "      <td>{'classifier__classifier__n_estimators': 400, 'classifier__classifier__min_samples_split': 2, 'classifier__classifier__min_samples_leaf': 2, 'classifier__classifier__max_features': 'auto', 'classifier__classifier__bootstrap': True}</td>\n",
              "      <td>0.723399</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "rdPWKCRCu9Hg",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "### Boosting Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "5F5Imj-Iu9Hg",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "\n",
        "\n",
        "XGBoost - Fine Tune\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "loGx-6NOu9Hg",
        "pycharm": {
          "name": "#%%\n"
        },
        "colab": {}
      },
      "source": [
        "# !wget https://s3-us-west-2.amazonaws.com/xgboost-wheels/xgboost-0.81-py2.py3-none-manylinux1_x86_64.whl\n",
        "# !pip uninstall xgboost --yes\n",
        "# !pip install xgboost-0.81-py2.py3-none-manylinux1_x86_64.whl\n",
        "\n",
        "# For Google Colab Only"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "02VNoY6ku9Hg",
        "pycharm": {
          "name": "#%%\n"
        },
        "colab": {}
      },
      "source": [
        "from xgboost import XGBClassifier"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "aG-WP81-u9Hg",
        "pycharm": {
          "name": "#%%\n"
        },
        "colab": {}
      },
      "source": [
        "\n",
        "param_grid = {\n",
        "        'classifier__classifier__colsample_bytree': [0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0],\n",
        "        'classifier__classifier__colsample_bylevel': [0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0],\n",
        "        'classifier__classifier__gamma': [0, 0.25, 0.5, 1.0]}\n",
        "\n",
        "\n",
        "xg_boost = Pipeline([\n",
        "    ('classifier', XGBClassifier(objective = 'multi:softmax', num_class=  3, tree_method='exact', subsample=0.1 ))\n",
        "])\n",
        "\n",
        "pipeline = Pipeline([\n",
        "    ('vectoriser', tfidf_vectoriser),\n",
        "    ('classifier', xg_boost)\n",
        "])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "nC45fzhtu9Hg",
        "outputId": "87d3d28e-01df-43d9-d75a-1f5f4a9cb0ea",
        "pycharm": {
          "name": "#%%\n"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        }
      },
      "source": [
        "rs_clf = RandomizedSearchCV(pipeline, param_grid, n_iter=10,\n",
        "                            n_jobs=1, verbose=2, cv=5,\n",
        "                            scoring='neg_log_loss', refit=False, random_state=42)\n",
        "results = train_predict_all(rs_clf)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
            "[CV] classifier__classifier__gamma=1.0, classifier__classifier__colsample_bytree=1.0, classifier__classifier__colsample_bylevel=0.8 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n",
            "  'stop_words.' % sorted(inconsistent))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-2f9461ffcfab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m                             \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                             scoring='neg_log_loss', refit=False, random_state=42)\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_predict_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrs_clf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-7-d2a2eaa6c9e6>\u001b[0m in \u001b[0;36mtrain_predict_all\u001b[0;34m(pipeline)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtrain_predict_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpipeline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcv_results_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     results_df = pd.DataFrame({'rank': out['rank_test_score'],\n\u001b[1;32m     13\u001b[0m                           \u001b[0;34m'params'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'params'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    708\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    709\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 710\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    711\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    712\u001b[0m         \u001b[0;31m# For multi-metric evaluation, store the best_index_, best_params_ and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1482\u001b[0m         evaluate_candidates(ParameterSampler(\n\u001b[1;32m   1483\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_distributions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1484\u001b[0;31m             random_state=self.random_state))\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params)\u001b[0m\n\u001b[1;32m    687\u001b[0m                                \u001b[0;32mfor\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m                                in product(candidate_params,\n\u001b[0;32m--> 689\u001b[0;31m                                           cv.split(X, y, groups)))\n\u001b[0m\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    691\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1002\u001b[0m             \u001b[0;31m# remaining jobs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1003\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1004\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1005\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    833\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    834\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 835\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    836\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    837\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    752\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 754\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    755\u001b[0m             \u001b[0;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    756\u001b[0m             \u001b[0;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    207\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    210\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    588\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    589\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 590\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    591\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    254\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 256\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    254\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 256\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, error_score)\u001b[0m\n\u001b[1;32m    513\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    514\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 515\u001b[0;31m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    516\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    517\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    352\u001b[0m                                  self._log_message(len(self.steps) - 1)):\n\u001b[1;32m    353\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'passthrough'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 354\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    355\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    352\u001b[0m                                  self._log_message(len(self.steps) - 1)):\n\u001b[1;32m    353\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'passthrough'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 354\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    355\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/xgboost/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, callbacks)\u001b[0m\n\u001b[1;32m    730\u001b[0m                               \u001b[0mevals_result\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevals_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    731\u001b[0m                               \u001b[0mverbose_eval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxgb_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mxgb_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 732\u001b[0;31m                               callbacks=callbacks)\n\u001b[0m\u001b[1;32m    733\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    734\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobjective\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxgb_options\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"objective\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/xgboost/training.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, learning_rates)\u001b[0m\n\u001b[1;32m    214\u001b[0m                            \u001b[0mevals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m                            \u001b[0mobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m                            xgb_model=xgb_model, callbacks=callbacks)\n\u001b[0m\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/xgboost/training.py\u001b[0m in \u001b[0;36m_train_internal\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, xgb_model, callbacks)\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;31m# Skip the first update if it is a recovery step.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mversion\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m             \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m             \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_rabit_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0mversion\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/xgboost/core.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[1;32m   1107\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1108\u001b[0m             _check_call(_LIB.XGBoosterUpdateOneIter(self.handle, ctypes.c_int(iteration),\n\u001b[0;32m-> 1109\u001b[0;31m                                                     dtrain.handle))\n\u001b[0m\u001b[1;32m   1110\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1111\u001b[0m             \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_ImBrPx9u9Hg",
        "pycharm": {
          "is_executing": false,
          "name": "#%% md\n"
        }
      },
      "source": [
        "AdaBoost\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "rBw3bi62u9Hg",
        "pycharm": {
          "name": "#%%\n"
        },
        "colab": {}
      },
      "source": [
        "from sklearn.ensemble import AdaBoostClassifier\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "dXZXdSYFu9Hg",
        "pycharm": {
          "name": "#%%\n"
        },
        "colab": {}
      },
      "source": [
        "\n",
        "adaBoost = Pipeline([\n",
        "    ('classifier', AdaBoostClassifier()),\n",
        "])\n",
        "\n",
        "pipeline = Pipeline([\n",
        "    ('vectoriser', tfidf_vectoriser),\n",
        "    ('classifier', adaBoost)\n",
        "])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tLpkO8nEu9Hg",
        "outputId": "30f83c56-bf3a-46fc-da2b-1a37b6f540be",
        "pycharm": {
          "name": "#%%\n"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 280
        }
      },
      "source": [
        "train_predict(pipeline)\n",
        "results = train_predict_all(gs_mnb)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         1.0       0.19      0.05      0.08      6581\n",
            "         2.0       0.21      0.09      0.12      6641\n",
            "         3.0       0.20      0.71      0.31      6575\n",
            "         4.0       0.20      0.03      0.06      6573\n",
            "         5.0       0.22      0.13      0.17      6572\n",
            "\n",
            "    accuracy                           0.20     32942\n",
            "   macro avg       0.21      0.20      0.15     32942\n",
            "weighted avg       0.21      0.20      0.15     32942\n",
            "\n",
            "f1 score is 0.1455947998473248, accuracy is 0.2014449638759031\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "W-WkjA5eu9Hg",
        "outputId": "ac548d25-8794-4a74-f8b2-3fdd3e5359b5",
        "pycharm": {
          "is_executing": true,
          "name": "#%%\n"
        },
        "colab": {}
      },
      "source": [
        "train_predict(pipeline)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/home/alex/anaconda3/envs/deepLearning/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "vLRH790Hxf4s",
        "colab_type": "code",
        "outputId": "f180d490-ff02-484b-9eca-f602b91d0349",
        "colab": {}
      },
      "source": [
        "from pprint import pprint\n",
        "pprint(gs_mnb.estimator.get_params().keys())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dict_keys(['memory', 'steps', 'verbose', 'vectoriser', 'classifier', 'vectoriser__memory', 'vectoriser__steps', 'vectoriser__verbose', 'vectoriser__tfidfVectoriser', 'vectoriser__tfidfVectoriser__analyzer', 'vectoriser__tfidfVectoriser__binary', 'vectoriser__tfidfVectoriser__decode_error', 'vectoriser__tfidfVectoriser__dtype', 'vectoriser__tfidfVectoriser__encoding', 'vectoriser__tfidfVectoriser__input', 'vectoriser__tfidfVectoriser__lowercase', 'vectoriser__tfidfVectoriser__max_df', 'vectoriser__tfidfVectoriser__max_features', 'vectoriser__tfidfVectoriser__min_df', 'vectoriser__tfidfVectoriser__ngram_range', 'vectoriser__tfidfVectoriser__norm', 'vectoriser__tfidfVectoriser__preprocessor', 'vectoriser__tfidfVectoriser__smooth_idf', 'vectoriser__tfidfVectoriser__stop_words', 'vectoriser__tfidfVectoriser__strip_accents', 'vectoriser__tfidfVectoriser__sublinear_tf', 'vectoriser__tfidfVectoriser__token_pattern', 'vectoriser__tfidfVectoriser__tokenizer', 'vectoriser__tfidfVectoriser__use_idf', 'vectoriser__tfidfVectoriser__vocabulary', 'classifier__memory', 'classifier__steps', 'classifier__verbose', 'classifier__classifier', 'classifier__classifier__C', 'classifier__classifier__class_weight', 'classifier__classifier__dual', 'classifier__classifier__fit_intercept', 'classifier__classifier__intercept_scaling', 'classifier__classifier__l1_ratio', 'classifier__classifier__max_iter', 'classifier__classifier__multi_class', 'classifier__classifier__n_jobs', 'classifier__classifier__penalty', 'classifier__classifier__random_state', 'classifier__classifier__solver', 'classifier__classifier__tol', 'classifier__classifier__verbose', 'classifier__classifier__warm_start'])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0O88emmgqhSs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}