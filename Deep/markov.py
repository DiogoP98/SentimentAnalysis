# -*- coding: utf-8 -*-
"""Copy of 7_1_SequenceModelling.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1LD86DzGYqCr1uNjZcLDhglvlYiPJXCuj

# Part 1: Sequence Modelling

__Before starting, we recommend you enable GPU acceleration if you're running on Colab.__
"""

# Execute this code block to install dependencies when running on colab

"""## Markov chains

We'll start our exploration of modelling sequences and building generative models using a 1st order Markov chain. The Markov chain is a stochastic model describing a sequence of possible events in which the probability of each event depends only on the state attained in the previous event. In our case we're going to learn a model over a set of characters from an English language text. The events, or states, in our model are the set of possible characters, and we'll learn the probability of moving from one character to the next.

Let's start by loading the data from the web:
"""

from torchvision.datasets.utils import download_url
import torch
import pandas as pd
import random
import sys
import io
import numpy as np
from numpy import save


# Read the data


# text = io.open('Datasets\\new_clean_sm.csv', encoding='utf-8').read().lower()
df = pd.read_csv(r"Datasets\\new_clean_sm.csv")

print(df.shape)
temp = df[['reviewText', 'overall']]

maxlen = 40
step = 3
try:
    ones = np.load(r'Datasets\ones.npy', allow_pickle=True)
    twos = np.load(r'Datasets\twos.npy', allow_pickle=True)
    threes = np.load(r'Datasets\threes.npy', allow_pickle=True)
    fours = np.load(r'Datasets\fours.npy', allow_pickle=True)
    fives = np.load(r'Datasets\fives.npy', allow_pickle=True)
except:
    ones = temp.loc[temp['overall'].isin([1])]
    twos = temp.loc[temp['overall'].isin([2])]
    threes = temp.loc[temp['overall'].isin([3])]
    fours = temp.loc[temp['overall'].isin([4])]
    fives = temp.loc[temp['overall'].isin([5])]
    ones = ones['reviewText'].values
    twos = twos['reviewText'].values
    threes = threes['reviewText'].values
    fours = fours['reviewText'].values
    fives = fives['reviewText'].values
    save(r'Datasets\ones.npy', ones)
    save(r'Datasets\twos.npy', twos)
    save(r'Datasets\threes.npy', threes)
    save(r'Datasets\fours.npy', fours)
    save(r'Datasets\fives.npy', fives)

try:
    oneswords =  np.load(r'Datasets\oneswords.npy'   , allow_pickle=True)
    twoswords =  np.load(r'Datasets\twoswords.npy'   , allow_pickle=True)
    threeswords= np.load(r'Datasets\threeswords.npy' , allow_pickle=True)
    fourswords = np.load(r'Datasets\fourswords.npy'  , allow_pickle=True)
    fiveswords = np.load(r'Datasets\fiveswords.npy'  , allow_pickle=True)
except:
    oneswords = ''
    twoswords= ''
    threeswords= ''
    fourswords= ''
    fiveswords= ''
    print(ones[0])
    for reviews in ones:
        oneswords=  oneswords+str(reviews)
    oneswords = oneswords.split(' ')
    for reviews in twos:
        twoswords= twoswords+str(reviews)
    twoswords = twoswords.split(' ')
    for reviews in threes:
        threeswords=threeswords+str(reviews)
    threeswords = threeswords.split(' ')
    for reviews in fours:
        fourswords=fourswords+str(reviews)
    fourswords = fourswords.split(' ')
    for reviews in fives:
        fiveswords=  fiveswords+str(reviews)
    fiveswords = fiveswords.split(' ')

    save(r'Datasets\oneswords.npy'  , oneswords)
    save(r'Datasets\twoswords.npy'  , twoswords)
    save(r'Datasets\threeswords.npy', threeswords)
    save(r'Datasets\fourswords.npy' , fourswords)
    save(r'Datasets\fiveswords.npy' , fiveswords)

text = oneswords


oneswords = [s+' 'for s in oneswords]
oneswords[-1] = ' '
"""We now need to iterate over the characters in the text and count the times each transition happens:"""

#transition_counts = dict()
#for i in range(0,len(text)-1):
#    currc = text[i]
#    nextc = text[i+1]
#    if currc not in transition_counts:
#        transition_counts[currc] = dict()
#    if nextc not in transition_counts[currc]:
#        transition_counts[currc][nextc] = 0
#    transition_counts[currc][nextc] += 1
#
#"""The `transition_counts` dictionary maps the current character to the next character, and this is then mapped to a count. We can for example use this datastructure to get the number of times the letter 'a' was followed by a 'b':"""
#
#print("Number of transitions from 'a' to 'b': " + str(transition_counts['a']['b']))
#
#"""Finally, to complete the model we need to normalise the counts for each initial character into a probability distribution over the possible next character. We'll slightly modify the form we're storing these and maintain a tuple of array objects for each initial character: the first holding the set of possible characters, and the second holding the corresponding probabilities:"""
#
#transition_probabilities = dict()
#for currentc, next_counts in transition_counts.items():
#    values = []
#    probabilities = []
#    sumall = 0
#    for nextc, count in next_counts.items():
#        values.append(nextc)
#        probabilities.append(count)
#        sumall += count
#    for i in range(0, len(probabilities)):
#        probabilities[i] /= float(sumall)
#    transition_probabilities[currentc] = (values, probabilities)
#
#"""At this point, we could print out the probability distribution for a given initial character state. For example, to print the distribution for 'a':"""
#
#for a,b in zip(transition_probabilities['a'][0], transition_probabilities['a'][1]):
#    print(a,b)
#
#"""It looks like the most probable letter to follow an 'a' is 'n'.
#
#__What is the most likely letter to follow the letter 'j'? Write your answer in the block below:__
#"""
#
#letter = 'the'
#print(max(transition_probabilities[letter][1]))
#print(transition_probabilities[letter][1].index(max(transition_probabilities[letter][1])))
#transition_probabilities[letter][0][transition_probabilities[letter][1].index(max(transition_probabilities[letter][1]))]
#
#"""We mentioned earlier that the Markov model is generative. This means that we can draw samples from the distributions and iteratively move between states.
#
#Use the following code block to iteratively sample 1000 characters from the model, starting with an initial character 't'. You can use the `torch.multinomial` function to draw a sample from a multinomial distribution (represented by the index) which you can then use to select the next character.
#"""
#
#current = 't'
#for i in range(0, 1000):
#    print(current, end='')
#    # sample the next character based on `current` and store the result in `current`
#    p = torch.multinomial(torch.tensor(transition_probabilities[current][1]),1).item()
#    current = transition_probabilities[current][0][p]
#    print(p)
#
#"""You should observe a result that is clearly not English, but it should be obvious that some of the common structures in the English language have been captured.
#
#__Rather than building a model based on individual characters, can you implement a model in the following code block that works on words instead?__
#"""
#
##print(text)
#import re
#
##ttext=re.findall(r'\w+', text)
#ttext=text.split(' ')
#print(ttext)
#print(len(ttext))
#transition_counts = dict()
#for i in range(0,len(ttext)-1):
#    currc = ttext[i]
#    nextc = ttext[i+1]
#    if currc not in transition_counts:
#        transition_counts[currc] = dict()
#    if nextc not in transition_counts[currc]:
#        transition_counts[currc][nextc] = 0
#    transition_counts[currc][nextc] += 1
#
#transition_probabilities = dict()
#for currentc, next_counts in transition_counts.items():
#    values = []
#    probabilities = []
#    sumall = 0
#    for nextc, count in next_counts.items():
#        values.append(nextc)
#        probabilities.append(count)
#        sumall += count
#    for i in range(0, len(probabilities)):
#        probabilities[i] /= float(sumall)
#    transition_probabilities[currentc] = (values, probabilities)
#
#
#
#
#current = 'the'
#for i in range(0, 1000):
#    print(current, end=' ')
#    # sample the next character based on `current` and store the result in `current`
#    p = torch.multinomial(torch.tensor(transition_probabilities[current][1]),1).item()
#    current = transition_probabilities[current][0][p]
#    #print(p)
#
#"""## RNN-based sequence modelling
#
#It is possible to build higher-order Markov models that capture longer-term dependencies in the text and have higher accuracy, however this does tend to become computationally infeasible very quickly. Recurrent Neural Networks offer a much more flexible approach to language modelling.
#
#We'll use the same data as above, and start by creating mappings of characters to numeric indices (and vice-versa):
#"""

chars = oneswords

print('total chars:', len(chars))
char_indices = dict((c, i) for i, c in enumerate(chars))
indices_char = dict((i, c) for i, c in enumerate(chars))

"""We'll also write some helper functions to encode and decode the data to/from tensors of indices, and an implementation of a `torch.Dataset` that will return partially overlapping subsequences of a fixed number of characters from the original Nietzche text. Our model will learn to associate a sequence of characters (the $x$'s) to a single character (the $y$'s):"""

from torch.utils.data import Dataset, DataLoader
from torch import nn
from torch.nn import functional as F
from torch import optim
import random
import sys
import io

maxlen = 40
step = 3


def encode(inp):
    # encode the characters in a tensor
    inp = inp.split(sep=' ')
    inp = [s+' 'for s in inp]
    x = torch.zeros(maxlen, dtype=torch.long)
    for t, char in enumerate(inp):
        x[t] = char_indices[char]

    return x


def decode(ten):
    s = ''
    for v in ten:
        s += indices_char[v]
    return s


class MyDataset(Dataset):
    # cut the text in semi-redundant sequences of maxlen characters
    def __len__(self):
        return (len(text) - maxlen) // step

    def __getitem__(self, i):
        inp = text[i*step: i*step + maxlen]
        out = text[i*step + maxlen]

        x = encode(inp)
        y = char_indices[out]

        return x, y

"""We can now define the model. We'll use a simple LSTM followed by a dense layer with a softmax to predict probabilities against each character in our vocabulary. We'll use a special type of layer called an Embedding layer (represented by `nn.Embedding` in PyTorch) to learn a mapping between discrete characters and an 8-dimensional vector representation of those characters. You'll learn more about Embeddings in the next part of the lab."""

class CharPredictor(nn.Module):
    def __init__(self):
        super(CharPredictor, self).__init__()
        self.emb = nn.Embedding(len(chars), 8)
        self.lstm = nn.LSTM(8, 128, batch_first=True)
        self.lin = nn.Linear(128, len(chars))

    def forward(self, x):
        x = self.emb(x)
        lstm_out, _ = self.lstm(x)
        out = self.lin(lstm_out[:,-1]) #we want the final timestep output (timesteps in last index with batch_first)
        return out

"""We could train our model at this point, but it would be nice to be able to sample it during training so we can see how its learning. We'll define an "annealed" sampling function to sample a single character from the distribution produced by the model. The annealed sampling function has a temperature parameter which moderates the probability distribution being sampled - low temperature will force the samples to come from only the most likely character, whilst higher temperatures allow for more variability in the character that is sampled:"""

def sample(logits, temperature=1.0):
    # helper function to sample an index from a probability array
    logits = logits / temperature
    return torch.multinomial(F.softmax(logits, dim=0), 1)

"""Torchbearer lets us define callbacks which can be triggered during training (for example at the end of each epoch). Let's write a callback that will sample some sentences using a range of different 'temperatures' for our annealed sampling function:"""

import torchbearer as tb
from torchbearer import Trial
from torchbearer.callbacks.decorators import on_end_epoch

device = "cuda:0" if torch.cuda.is_available() else "cpu"

@on_end_epoch
def create_samples(state):
    with torch.no_grad():
        epoch = -1
        if state is not None:
            epoch = state[tb.EPOCH]

        print()
        print('----- Generating text after Epoch: %d' % epoch)

        start_index = random.randint(0, len(text) - maxlen - 1)
        for diversity in [0.2, 0.5, 1.0, 1.2]:
            print()
            print()
            print('----- diversity:', diversity)

            generated = ''
            sentence = text[start_index:start_index+maxlen-1]

            generated += sentence
            print('----- Generating with seed: "' + sentence + '"')
            print()
            sys.stdout.write(generated)

            inputs = encode(sentence).unsqueeze(0).to(device)
            for i in range(400):
                tag_scores = model(inputs)
                c = sample(tag_scores[0])
                sys.stdout.write(indices_char[c.item()])
                sys.stdout.flush()
                inputs[0, 0:inputs.shape[1]-1] = inputs[0, 1:].clone()
                inputs[0, inputs.shape[1]-1] = c
        print()

"""Now, all the pieces are in place. __Use the following block to:__

- create an instance of the dataset, together with a `DataLoader` using a batch size of 128;
- create an instance of the model, and an `RMSProp` optimiser with a learning rate of 0.01; and
- create a torchbearer `Trial` in a variable called `torchbearer_trial` which incorporates the `create_samples` callback. Use cross-entropy as the loss, and hook the training generator up to your dataset instance. Make sure you move your `Trial` object to the GPU if one is available.
"""

# YOUR CODE HERE

# create data loaders

#train_data = MyDataset()
#val_data = MyDataset()
#test_data = MyDataset()
#
#trainloader = DataLoader(train_data, batch_size=128, shuffle=True)
#testloader = DataLoader(test_data, batch_size=128, shuffle=True)
#valloader = DataLoader(val_data, batch_size=128, shuffle=True)
#
#model = CharPredictor()
#torch.manual_seed(7)
## define the loss function and the optimiser
#loss_function = nn.cro
#optimiser = optim.RMSprop(model.parameters(),lr=0.01)
#print(device)
#trial = Trial(model, optimiser, loss_function, metrics=['loss', 'accuracy'], callbacks=[create_samples]).to(device)
#trial.with_generators(trainloader, test_generator=testloader, val_generator=valloader )
#tem = trial.run(epochs=10)
data = MyDataset()
train_loader = DataLoader(data, batch_size=128, shuffle=True)

model = CharPredictor()
loss_function = nn.CrossEntropyLoss()
optimiser = optim.RMSprop(model.parameters(), lr=0.01)

device = "cuda:0" if torch.cuda.is_available() else "cpu"
trial = Trial(model, optimiser, loss_function, metrics=['loss', 'accuracy'], callbacks=[create_samples]).to(device)
trial.with_generators(train_loader)
create_samples.on_end_epoch(None)
trial.run(epochs=10)

"""Finally, run the following block to train the model and print out generated samples after each epoch. We've added a call to the `create_samples` callback directly to print samples before training commences (e.g. with random weights). Be aware this will take some time to run..."""

#create_samples.on_end_epoch(None)
#torchbearer_trial.run(epochs=10)

"""Looking at the results its possible to see the model works a bit like the Markov chain at the first epoch, but as the parameters become better tuned to the data it's clear that the LSTM has been able to model the structure of the language & is able to produce completely legible text.

__Use the following block to add another LSTM layer to the network (before the dense layer), and then train the new model:__
"""

# YOUR CODE HERE
raise NotImplementedError()

"""__How does the additional layer affect performance of the model? Provide your answer in the block below:__

YOUR ANSWER HERE
"""