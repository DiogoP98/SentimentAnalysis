{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TextCNN.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "jdw4MOBVGsec",
        "colab_type": "code",
        "outputId": "d849cae7-765a-47b5-cd72-0ae5633ee202",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "!pip install torchbearer\n",
        "!pip install progress"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torchbearer in /usr/local/lib/python3.6/dist-packages (0.5.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchbearer) (1.18.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from torchbearer) (4.41.1)\n",
            "Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from torchbearer) (1.5.0+cu101)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=1.0.0->torchbearer) (0.16.0)\n",
            "Requirement already satisfied: progress in /usr/local/lib/python3.6/dist-packages (1.5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DJ6xy-PAEMAI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, random_split, DataLoader\n",
        "import numpy as np\n",
        "import time\n",
        "import datetime\n",
        "from torch import nn\n",
        "import sys\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "import torchtext.vocab\n",
        "from torchtext import data\n",
        "import pandas as pd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o_wHozmeEQUs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torchtext.data import Field, Dataset, Example\n",
        "import pandas as pd\n",
        "\n",
        "class DataFrameDataset(Dataset):\n",
        "    \"\"\"Class for using pandas DataFrames as a datasource\"\"\"\n",
        "\n",
        "    def __init__(self, examples, fields, filter_pred=None):\n",
        "        \"\"\"\n",
        "        Create a dataset from a pandas dataframe of examples and Fields\n",
        "        Arguments:\n",
        "            examples pd.DataFrame: DataFrame of examples\n",
        "            fields {str: Field}: The Fields to use in this tuple. The\n",
        "                string is a field name, and the Field is the associated field.\n",
        "            filter_pred (callable or None): use only exanples for which\n",
        "                filter_pred(example) is true, or use all examples if None.\n",
        "                Default is None\n",
        "        \"\"\"\n",
        "        self.examples = examples.apply(SeriesExample.fromSeries, args=(fields,), axis=1).tolist()\n",
        "        if filter_pred is not None:\n",
        "            self.examples = filter(filter_pred, self.examples)\n",
        "        self.fields = dict(fields)\n",
        "        # Unpack field tuples\n",
        "        for n, f in list(self.fields.items()):\n",
        "            if isinstance(n, tuple):\n",
        "                self.fields.update(zip(n, f))\n",
        "                del self.fields[n]\n",
        "\n",
        "\n",
        "class SeriesExample(Example):\n",
        "    \"\"\"Class to convert a pandas Series to an Example\"\"\"\n",
        "\n",
        "    @classmethod\n",
        "    def fromSeries(cls, data, fields):\n",
        "        return cls.fromdict(data.to_dict(), fields)\n",
        "\n",
        "    @classmethod\n",
        "    def fromdict(cls, data, fields):\n",
        "        ex = cls()\n",
        "\n",
        "        for key, field in fields.items():\n",
        "            if key not in data:\n",
        "                raise ValueError(\"Specified key {} was not found in \"\n",
        "                                 \"the input data\".format(key))\n",
        "            if field is not None:\n",
        "                setattr(ex, key, field.preprocess(data[key]))\n",
        "            else:\n",
        "                setattr(ex, key, data[key])\n",
        "\n",
        "\n",
        "        return ex\n",
        "\n",
        "def three_class_problem(df):\n",
        "  \n",
        "  df = df[df['overall'] != 2]\n",
        "  df = df[df['overall'] != 4]\n",
        "  df.loc[df['overall'] == 1, 'overall'] = 0\n",
        "  df.loc[df['overall'] == 3, 'overall'] = 1\n",
        "  df.loc[df['overall'] == 5, 'overall'] = 2\n",
        "\n",
        "  return df\n",
        "\n",
        "\n",
        "\n",
        "# def create_iterator(train_data, valid_data, test_data, batch_size, device):\n",
        "#     #  BucketIterator : Defines an iterator that batches examples of similar lengths together to minimize the amount of padding needed.\n",
        "#     # by setting sort_within_batch = True.\n",
        "#     train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits((train_data, valid_data, test_data),\n",
        "#         batch_size = batch_size,\n",
        "#         sort_key = lambda x: len(x.reviewText), # Sort the batches by text length size\n",
        "#         sort_within_batch = True,\n",
        "#         device = device)\n",
        "#     return train_iterator, valid_iterator, test_iterator"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZeRkK9duETQM",
        "colab_type": "code",
        "outputId": "ada98d33-289b-479a-cf66-5750ecfcb749",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "root_path = \"/content/drive/My Drive/notebooks\"\n",
        "# For Google colab only\n",
        "\n",
        "df = pd.read_csv(f\"{root_path}/new_clean_sm_100000.csv\")\n",
        "df = df[df['reviewText'].notna()]\n",
        "df = df[~df['reviewText'].str.contains(\".jpg|.png|.jpeg|.tiff|.gif|.bmp|.heif\", regex=True, na=False)]\n",
        "df = three_class_problem(df)\n",
        "#df[\"overall\"] = df[\"overall\"].apply(lambda x: x - 1)\n",
        "train_df, validate_df, test_df = np.split(df.sample(frac=1), [int(.6*len(df)), int(.8*len(df))])\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9rtwXUWOETi8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# credit to https://github.com/Shawn1993/cnn-text-classification-pytorch for the TextCNN model\n",
        "\n",
        "class CNN_Text(nn.Module):\n",
        "    \n",
        "    def __init__(self, embed_num, embed_dim, class_num, kernel_num,kernel_sizes):\n",
        "        super(CNN_Text, self).__init__()\n",
        "       \n",
        "        \n",
        "        V = embed_num\n",
        "        D = embed_dim\n",
        "        C = class_num\n",
        "        Ci = 1\n",
        "        Co = kernel_num\n",
        "        Ks = kernel_sizes\n",
        "\n",
        "        self.embed = nn.Embedding(V, D)\n",
        "        # self.convs1 = [nn.Conv2d(Ci, Co, (K, D)) for K in Ks]\n",
        "        self.convs1 = nn.ModuleList([nn.Conv2d(Ci, Co, (K, D)) for K in Ks])\n",
        "        '''\n",
        "        self.conv13 = nn.Conv2d(Ci, Co, (3, D))\n",
        "        self.conv14 = nn.Conv2d(Ci, Co, (4, D))\n",
        "        self.conv15 = nn.Conv2d(Ci, Co, (5, D))\n",
        "        '''\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "        self.fc1 = nn.Linear(len(Ks)*Co, C)\n",
        "\n",
        "    def conv_and_pool(self, x, conv):\n",
        "        x = F.relu(conv(x)).squeeze(3)  # (N, Co, W)\n",
        "        x = F.max_pool1d(x, x.size(2)).squeeze(2)\n",
        "        return x\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embed(x)  # (N, W, D)\n",
        "        \n",
        "        #if self.args.static:\n",
        "         #   x = Variable(x)\n",
        "\n",
        "        x = x.unsqueeze(1)  # (N, Ci, W, D)\n",
        "\n",
        "        x = [F.relu(conv(x)).squeeze(3) for conv in self.convs1]  # [(N, Co, W), ...]*len(Ks)\n",
        "\n",
        "        x = [F.max_pool1d(i, i.size(2)).squeeze(2) for i in x]  # [(N, Co), ...]*len(Ks)\n",
        "\n",
        "        x = torch.cat(x, 1)\n",
        "\n",
        "        '''\n",
        "        x1 = self.conv_and_pool(x,self.conv13) #(N,Co)\n",
        "        x2 = self.conv_and_pool(x,self.conv14) #(N,Co)\n",
        "        x3 = self.conv_and_pool(x,self.conv15) #(N,Co)\n",
        "        x = torch.cat((x1, x2, x3), 1) # (N,len(Ks)*Co)\n",
        "        '''\n",
        "        x = self.dropout(x)  # (N, len(Ks)*Co)\n",
        "        logit = self.fc1(x)  # (N, C)\n",
        "        return logit"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yRvWP_x0FgKE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#df, num_classes = three_class_problem(df)\n",
        "max_document_length = 100  # each sentence has until 100 words\n",
        "max_size = 5000 # maximum vocabulary size\n",
        "\n",
        "Text = data.Field(tokenize='spacy', batch_first=True, include_lengths=True, fix_length=max_document_length) # fix_length - make the sentences padded in the same lengths for all the batches\n",
        "Label = data.Field(sequential=False, use_vocab=False, pad_token=None, unk_token=None)\n",
        "fields = { 'overall' : Label, 'reviewText' : Text }\n",
        "train_ds = DataFrameDataset(train_df, fields)\n",
        "test_ds = DataFrameDataset(test_df, fields)\n",
        "valid_ds = DataFrameDataset(validate_df, fields)\n",
        "\n",
        "Text.build_vocab(train_ds, max_size=max_size, vectors=\"glove.6B.100d\")\n",
        "Label.build_vocab(train_ds)\n",
        "vocab_size = len(Text.vocab)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ukaB0Cr3dXYk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(train_iter, dev_iter, model):\n",
        " \n",
        "    device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "    model = model.to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "\n",
        "    steps = 0\n",
        "    best_acc = 0\n",
        "    last_step = 0\n",
        "    model.train()\n",
        "    for epoch in tqdm(range(1, 2+0)):\n",
        "        for batch in train_iter:\n",
        "            #feature.data.t_(), target.data.sub_(1)  # batch first, index align\n",
        "\n",
        "            feature, _ =  batch.reviewText\n",
        "            target = batch.overall\n",
        "            feature, target = feature.to(device), target.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            logit = model(feature)\n",
        "\n",
        "            #print('logit vector', logit.size())\n",
        "            #print('target vector', target.size())\n",
        "            loss = F.cross_entropy(logit, target)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            steps += 1\n",
        "            if steps % 100 == 0:\n",
        "                corrects = (torch.max(logit, 1)[1].view(target.size()).data == target.data).sum()\n",
        "                accuracy = 100.0 * corrects/batch.batch_size\n",
        "                print(\n",
        "                    '\\rBatch[{}] - loss: {:.6f}  acc: {:.4f}%({}/{})'.format(steps, \n",
        "                                                                             loss.item(), \n",
        "                                                                             accuracy,\n",
        "                                                                             corrects,\n",
        "                                                                             batch.batch_size))\n",
        "            if steps % 100 == 0:\n",
        "                dev_acc = eval(dev_iter, model)\n",
        "                if dev_acc > best_acc:\n",
        "                    best_acc = dev_acc\n",
        "                    last_step = steps\n",
        "                    #if args.save_best:\n",
        "                     #   save(model, args.save_dir, 'best', steps)\n",
        "                else:\n",
        "                    if steps - last_step >= 1000:\n",
        "                        print('early stop by {} steps.'.format(1000))\n",
        "            \n",
        "\n",
        "\n",
        "def eval(data_iter, model):\n",
        "    model.eval()\n",
        "    corrects, avg_loss = 0, 0\n",
        "    for batch in data_iter:\n",
        "      \n",
        "        #feature.data.t_(), target.data.sub_(1)  # batch first, index align\n",
        "\n",
        "        feature, _ =  batch.reviewText\n",
        "        target = batch.overall\n",
        "        feature, target = feature.to(device), target.to(device)\n",
        "\n",
        "\n",
        "        logit = model(feature)\n",
        "        loss = F.cross_entropy(logit, target, size_average=False)\n",
        "\n",
        "        avg_loss += loss.item()\n",
        "        corrects += (torch.max(logit, 1)\n",
        "                     [1].view(target.size()).data == target.data).sum()\n",
        "\n",
        "    size = len(data_iter.dataset)\n",
        "    avg_loss /= size\n",
        "    accuracy = 100.0 * corrects/size\n",
        "    print('\\nEvaluation - loss: {:.6f}  acc: {:.4f}%({}/{}) \\n'.format(avg_loss, \n",
        "                                                                       accuracy, \n",
        "                                                                       corrects, \n",
        "                                                                       size))\n",
        "    return accuracy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hb1x2CTFF5f8",
        "colab_type": "code",
        "outputId": "3914d472-12fa-433e-8f9a-ba860eef6f70",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from torch import optim\n",
        "\n",
        "embed_num = vocab_size\n",
        "embed_dim = 128\n",
        "class_num = 3\n",
        "kernel_num = 100\n",
        "kernel_sizes = [3,4,5]\n",
        "batch_size = 64\n",
        "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "class MyIter:\n",
        "    def __init__(self, it):\n",
        "        self.it = it\n",
        "    def __iter__(self):\n",
        "        for batch in self.it:\n",
        "            yield (batch.reviewText, batch.overall.unsqueeze(1))\n",
        "    def __len__(self):\n",
        "        return len(self.it)\n",
        "\n",
        "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
        "    (train_ds, valid_ds, test_ds), \n",
        "    batch_size=batch_size,\n",
        "    device=device,\n",
        "    sort_key=lambda x: len(x.reviewText),\n",
        "    sort_within_batch=True)\n",
        "\n",
        "\n",
        "cnn = CNN_Text(embed_num,embed_dim,class_num,kernel_num,kernel_sizes)\n",
        "train(train_iterator, valid_iterator, cnn)\n",
        "#train_iterator, valid_iterator, test_iterator = create_iterator(train_ds, valid_ds, test_ds, batch_size, device)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\rBatch[100] - loss: 1.106692  acc: 46.8750%(30/64)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/_reduction.py:43: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
            "  warnings.warn(warning.format(ret))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Evaluation - loss: 0.873621  acc: 59.2988%(35533/59922) \n",
            "\n",
            "Batch[200] - loss: 0.632004  acc: 78.1250%(50/64)\n",
            "\n",
            "Evaluation - loss: 0.787248  acc: 63.3957%(37988/59922) \n",
            "\n",
            "Batch[300] - loss: 0.963693  acc: 59.3750%(38/64)\n",
            "\n",
            "Evaluation - loss: 0.719998  acc: 68.0034%(40749/59922) \n",
            "\n",
            "Batch[400] - loss: 0.584370  acc: 75.0000%(48/64)\n",
            "\n",
            "Evaluation - loss: 0.765443  acc: 64.9494%(38919/59922) \n",
            "\n",
            "Batch[500] - loss: 0.548912  acc: 76.5625%(49/64)\n",
            "\n",
            "Evaluation - loss: 0.661266  acc: 70.9139%(42493/59922) \n",
            "\n",
            "Batch[600] - loss: 0.570111  acc: 78.1250%(50/64)\n",
            "\n",
            "Evaluation - loss: 0.666381  acc: 70.0778%(41992/59922) \n",
            "\n",
            "Batch[700] - loss: 0.433452  acc: 85.9375%(55/64)\n",
            "\n",
            "Evaluation - loss: 0.622018  acc: 73.0700%(43785/59922) \n",
            "\n",
            "Batch[800] - loss: 0.966142  acc: 57.8125%(37/64)\n",
            "\n",
            "Evaluation - loss: 0.611640  acc: 73.7392%(44186/59922) \n",
            "\n",
            "Batch[900] - loss: 0.737999  acc: 67.1875%(43/64)\n",
            "\n",
            "Evaluation - loss: 0.602040  acc: 73.9478%(44311/59922) \n",
            "\n",
            "Batch[1000] - loss: 0.490173  acc: 78.1250%(50/64)\n",
            "\n",
            "Evaluation - loss: 0.664068  acc: 71.3361%(42746/59922) \n",
            "\n",
            "Batch[1100] - loss: 0.578011  acc: 70.3125%(45/64)\n",
            "\n",
            "Evaluation - loss: 0.618001  acc: 73.1401%(43827/59922) \n",
            "\n",
            "Batch[1200] - loss: 0.578053  acc: 75.0000%(48/64)\n",
            "\n",
            "Evaluation - loss: 0.597487  acc: 74.4051%(44585/59922) \n",
            "\n",
            "Batch[1300] - loss: 0.660413  acc: 68.7500%(44/64)\n",
            "\n",
            "Evaluation - loss: 0.615454  acc: 73.0266%(43759/59922) \n",
            "\n",
            "Batch[1400] - loss: 0.520097  acc: 75.0000%(48/64)\n",
            "\n",
            "Evaluation - loss: 0.583143  acc: 74.9608%(44918/59922) \n",
            "\n",
            "Batch[1500] - loss: 0.431459  acc: 85.9375%(55/64)\n",
            "\n",
            "Evaluation - loss: 0.562663  acc: 76.0322%(45560/59922) \n",
            "\n",
            "Batch[1600] - loss: 0.723649  acc: 67.1875%(43/64)\n",
            "\n",
            "Evaluation - loss: 0.649741  acc: 72.5159%(43453/59922) \n",
            "\n",
            "Batch[1700] - loss: 0.529207  acc: 78.1250%(50/64)\n",
            "\n",
            "Evaluation - loss: 0.561280  acc: 76.0205%(45553/59922) \n",
            "\n",
            "Batch[1800] - loss: 0.632931  acc: 64.0625%(41/64)\n",
            "\n",
            "Evaluation - loss: 0.564513  acc: 76.2208%(45673/59922) \n",
            "\n",
            "Batch[1900] - loss: 0.708780  acc: 65.6250%(42/64)\n",
            "\n",
            "Evaluation - loss: 0.864742  acc: 62.4629%(37429/59922) \n",
            "\n",
            "Batch[2000] - loss: 0.489189  acc: 81.2500%(52/64)\n",
            "\n",
            "Evaluation - loss: 0.566673  acc: 75.4798%(45229/59922) \n",
            "\n",
            "Batch[2100] - loss: 0.460497  acc: 89.0625%(57/64)\n",
            "\n",
            "Evaluation - loss: 0.581213  acc: 75.5148%(45250/59922) \n",
            "\n",
            "Batch[2200] - loss: 0.585793  acc: 76.5625%(49/64)\n",
            "\n",
            "Evaluation - loss: 0.578322  acc: 75.2795%(45109/59922) \n",
            "\n",
            "Batch[2300] - loss: 0.311062  acc: 89.0625%(57/64)\n",
            "\n",
            "Evaluation - loss: 0.564565  acc: 76.2007%(45661/59922) \n",
            "\n",
            "Batch[2400] - loss: 0.559681  acc: 75.0000%(48/64)\n",
            "\n",
            "Evaluation - loss: 0.625532  acc: 73.6841%(44153/59922) \n",
            "\n",
            "Batch[2500] - loss: 0.412428  acc: 85.9375%(55/64)\n",
            "\n",
            "Evaluation - loss: 0.557031  acc: 76.0138%(45549/59922) \n",
            "\n",
            "Batch[2600] - loss: 0.548770  acc: 76.5625%(49/64)\n",
            "\n",
            "Evaluation - loss: 0.555238  acc: 76.1690%(45642/59922) \n",
            "\n",
            "Batch[2700] - loss: 0.568039  acc: 76.5625%(49/64)\n",
            "\n",
            "Evaluation - loss: 0.611077  acc: 74.0496%(44372/59922) \n",
            "\n",
            "Batch[2800] - loss: 0.308485  acc: 87.5000%(56/64)\n",
            "\n",
            "Evaluation - loss: 0.536105  acc: 77.2287%(46277/59922) \n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "100%|██████████| 1/1 [30:51<00:00, 1851.80s/it]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ED4Z5iTRGw_Q",
        "colab_type": "code",
        "outputId": "cd4d053a-6b41-4fef-cd79-3ae48ca4f7bc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "print(eval(test_iterator,cnn))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/_reduction.py:43: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
            "  warnings.warn(warning.format(ret))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Evaluation - loss: 0.525779  acc: 77.8278%(46636/59922) \n",
            "\n",
            "tensor(77.8278, device='cuda:0')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xxbA9oS_HRVk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "az2uey3AX01Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}