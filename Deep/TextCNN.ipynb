{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TextCNN.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "jdw4MOBVGsec",
        "colab_type": "code",
        "outputId": "d849cae7-765a-47b5-cd72-0ae5633ee202",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "!pip install torchbearer\n",
        "!pip install progress"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torchbearer in /usr/local/lib/python3.6/dist-packages (0.5.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchbearer) (1.18.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from torchbearer) (4.41.1)\n",
            "Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from torchbearer) (1.5.0+cu101)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=1.0.0->torchbearer) (0.16.0)\n",
            "Requirement already satisfied: progress in /usr/local/lib/python3.6/dist-packages (1.5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DJ6xy-PAEMAI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, random_split, DataLoader\n",
        "import numpy as np\n",
        "import time\n",
        "import datetime\n",
        "from torch import nn\n",
        "import sys\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "import torchtext.vocab\n",
        "from torchtext import data\n",
        "import pandas as pd\n",
        "from sklearn.metrics import f1_score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o_wHozmeEQUs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torchtext.data import Field, Dataset, Example\n",
        "import pandas as pd\n",
        "\n",
        "class DataFrameDataset(Dataset):\n",
        "    \"\"\"Class for using pandas DataFrames as a datasource\"\"\"\n",
        "\n",
        "    def __init__(self, examples, fields, filter_pred=None):\n",
        "        \"\"\"\n",
        "        Create a dataset from a pandas dataframe of examples and Fields\n",
        "        Arguments:\n",
        "            examples pd.DataFrame: DataFrame of examples\n",
        "            fields {str: Field}: The Fields to use in this tuple. The\n",
        "                string is a field name, and the Field is the associated field.\n",
        "            filter_pred (callable or None): use only exanples for which\n",
        "                filter_pred(example) is true, or use all examples if None.\n",
        "                Default is None\n",
        "        \"\"\"\n",
        "        self.examples = examples.apply(SeriesExample.fromSeries, args=(fields,), axis=1).tolist()\n",
        "        if filter_pred is not None:\n",
        "            self.examples = filter(filter_pred, self.examples)\n",
        "        self.fields = dict(fields)\n",
        "        # Unpack field tuples\n",
        "        for n, f in list(self.fields.items()):\n",
        "            if isinstance(n, tuple):\n",
        "                self.fields.update(zip(n, f))\n",
        "                del self.fields[n]\n",
        "\n",
        "\n",
        "class SeriesExample(Example):\n",
        "    \"\"\"Class to convert a pandas Series to an Example\"\"\"\n",
        "\n",
        "    @classmethod\n",
        "    def fromSeries(cls, data, fields):\n",
        "        return cls.fromdict(data.to_dict(), fields)\n",
        "\n",
        "    @classmethod\n",
        "    def fromdict(cls, data, fields):\n",
        "        ex = cls()\n",
        "\n",
        "        for key, field in fields.items():\n",
        "            if key not in data:\n",
        "                raise ValueError(\"Specified key {} was not found in \"\n",
        "                                 \"the input data\".format(key))\n",
        "            if field is not None:\n",
        "                setattr(ex, key, field.preprocess(data[key]))\n",
        "            else:\n",
        "                setattr(ex, key, data[key])\n",
        "\n",
        "\n",
        "        return ex\n",
        "\n",
        "def three_class_problem(df):\n",
        "  \n",
        "  df = df[df['overall'] != 2]\n",
        "  df = df[df['overall'] != 4]\n",
        "  df.loc[df['overall'] == 1, 'overall'] = 0\n",
        "  df.loc[df['overall'] == 3, 'overall'] = 1\n",
        "  df.loc[df['overall'] == 5, 'overall'] = 2\n",
        "\n",
        "  return df\n",
        "\n",
        "\n",
        "\n",
        "# def create_iterator(train_data, valid_data, test_data, batch_size, device):\n",
        "#     #  BucketIterator : Defines an iterator that batches examples of similar lengths together to minimize the amount of padding needed.\n",
        "#     # by setting sort_within_batch = True.\n",
        "#     train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits((train_data, valid_data, test_data),\n",
        "#         batch_size = batch_size,\n",
        "#         sort_key = lambda x: len(x.reviewText), # Sort the batches by text length size\n",
        "#         sort_within_batch = True,\n",
        "#         device = device)\n",
        "#     return train_iterator, valid_iterator, test_iterator"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZeRkK9duETQM",
        "colab_type": "code",
        "outputId": "f04f1f93-6604-4026-90e8-f7afdf9cef58",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "# root_path = \"/content/drive/My Drive/notebooks\"\n",
        "# # For Google colab only\n",
        "\n",
        "df = pd.read_csv(f\"../new_clean_sm_100000.csv\")\n",
        "df = df[df['reviewText'].notna()]\n",
        "df = df[~df['reviewText'].str.contains(\".jpg|.png|.jpeg|.tiff|.gif|.bmp|.heif\", regex=True, na=False)]\n",
        "df = three_class_problem(df)\n",
        "#df[\"overall\"] = df[\"overall\"].apply(lambda x: x - 1)\n",
        "train_df, validate_df, test_df = np.split(df.sample(frac=1), [int(.6*len(df)), int(.8*len(df))])\n",
        "\n",
        "\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9rtwXUWOETi8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# credit to https://github.com/Shawn1993/cnn-text-classification-pytorch for the TextCNN model\n",
        "\n",
        "class CNN_Text(nn.Module):\n",
        "    \n",
        "    def __init__(self, embed_num, embed_dim, class_num, kernel_num,kernel_sizes):\n",
        "        super(CNN_Text, self).__init__()\n",
        "       \n",
        "        \n",
        "        V = embed_num\n",
        "        D = embed_dim\n",
        "        C = class_num\n",
        "        Ci = 1\n",
        "        Co = kernel_num\n",
        "        Ks = kernel_sizes\n",
        "\n",
        "        self.embed = nn.Embedding(V, D)\n",
        "        # self.convs1 = [nn.Conv2d(Ci, Co, (K, D)) for K in Ks]\n",
        "        self.convs1 = nn.ModuleList([nn.Conv2d(Ci, Co, (K, D)) for K in Ks])\n",
        "        '''\n",
        "        self.conv13 = nn.Conv2d(Ci, Co, (3, D))\n",
        "        self.conv14 = nn.Conv2d(Ci, Co, (4, D))\n",
        "        self.conv15 = nn.Conv2d(Ci, Co, (5, D))\n",
        "        '''\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "        self.fc1 = nn.Linear(len(Ks)*Co, C)\n",
        "\n",
        "    def conv_and_pool(self, x, conv):\n",
        "        x = F.relu(conv(x)).squeeze(3)  # (N, Co, W)\n",
        "        x = F.max_pool1d(x, x.size(2)).squeeze(2)\n",
        "        return x\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embed(x)  # (N, W, D)\n",
        "        \n",
        "        #if self.args.static:\n",
        "         #   x = Variable(x)\n",
        "\n",
        "        x = x.unsqueeze(1)  # (N, Ci, W, D)\n",
        "\n",
        "        x = [F.relu(conv(x)).squeeze(3) for conv in self.convs1]  # [(N, Co, W), ...]*len(Ks)\n",
        "\n",
        "        x = [F.max_pool1d(i, i.size(2)).squeeze(2) for i in x]  # [(N, Co), ...]*len(Ks)\n",
        "\n",
        "        x = torch.cat(x, 1)\n",
        "\n",
        "        '''\n",
        "        x1 = self.conv_and_pool(x,self.conv13) #(N,Co)\n",
        "        x2 = self.conv_and_pool(x,self.conv14) #(N,Co)\n",
        "        x3 = self.conv_and_pool(x,self.conv15) #(N,Co)\n",
        "        x = torch.cat((x1, x2, x3), 1) # (N,len(Ks)*Co)\n",
        "        '''\n",
        "        x = self.dropout(x)  # (N, len(Ks)*Co)\n",
        "        logit = self.fc1(x)  # (N, C)\n",
        "        return logit"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yRvWP_x0FgKE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#df, num_classes = three_class_problem(df)\n",
        "max_document_length = 256  # each sentence has until 100 words\n",
        "max_size = 5000 # maximum vocabulary size\n",
        "\n",
        "Text = data.Field(tokenize='spacy', batch_first=True, include_lengths=True, fix_length=max_document_length) # fix_length - make the sentences padded in the same lengths for all the batches\n",
        "Label = data.Field(sequential=False, use_vocab=False, pad_token=None, unk_token=None)\n",
        "fields = { 'overall' : Label, 'reviewText' : Text }\n",
        "train_ds = DataFrameDataset(train_df, fields)\n",
        "test_ds = DataFrameDataset(test_df, fields)\n",
        "valid_ds = DataFrameDataset(validate_df, fields)\n",
        "\n",
        "Text.build_vocab(train_ds, max_size=max_size, vectors=\"glove.6B.100d\")\n",
        "Label.build_vocab(train_ds)\n",
        "vocab_size = len(Text.vocab)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ukaB0Cr3dXYk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(train_iter, dev_iter, model):\n",
        " \n",
        "    device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "    model = model.to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "\n",
        "    steps = 0\n",
        "    best_acc = 0\n",
        "    last_step = 0\n",
        "    model.train()\n",
        "    for epoch in (range(1, 2+0)):\n",
        "        for batch in tqdm(train_iter, total=len(train_iter)):\n",
        "            #feature.data.t_(), target.data.sub_(1)  # batch first, index align\n",
        "\n",
        "            feature, _ =  batch.reviewText\n",
        "            target = batch.overall\n",
        "            feature, target = feature.to(device), target.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            logit = model(feature)\n",
        "\n",
        "            #print('logit vector', logit.size())\n",
        "            #print('target vector', target.size())\n",
        "            loss = F.cross_entropy(logit, target)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            steps += 1\n",
        "            if steps % 100 == 0:\n",
        "                corrects = (torch.max(logit, 1)[1].view(target.size()).data == target.data).sum()\n",
        "                accuracy = 100.0 * corrects/batch.batch_size\n",
        "                print(\n",
        "                    '\\rBatch[{}] - loss: {:.6f}  acc: {:.4f}%({}/{})'.format(steps, \n",
        "                                                                             loss.item(), \n",
        "                                                                             accuracy,\n",
        "                                                                             corrects,\n",
        "                                                                             batch.batch_size))\n",
        "            if steps % 100 == 0:\n",
        "                dev_acc, valid_f1 = eval(dev_iter, model)\n",
        "                if dev_acc > best_acc:\n",
        "                    best_acc = dev_acc\n",
        "                    last_step = steps\n",
        "                    #if args.save_best:\n",
        "                     #   save(model, args.save_dir, 'best', steps)\n",
        "                else:\n",
        "                    if steps - last_step >= 1000:\n",
        "                        print('early stop by {} steps.'.format(1000))\n",
        "            \n",
        "\n",
        "\n",
        "def eval(data_iter, model):\n",
        "    model.eval()\n",
        "    corrects, avg_loss,  epoch_f1 = 0 , 0, 0\n",
        "    for batch in data_iter:\n",
        "      \n",
        "        #feature.data.t_(), target.data.sub_(1)  # batch first, index align\n",
        "\n",
        "        feature, _ =  batch.reviewText\n",
        "        target = batch.overall\n",
        "        feature, target = feature.to(device), target.to(device)\n",
        "\n",
        "\n",
        "        logit = model(feature)\n",
        "        loss = F.cross_entropy(logit, target, size_average=False)\n",
        "        f1 = f1_score(logit.argmax(dim=1).cpu().numpy(), batch.overall.cpu().numpy(), average='macro')\n",
        "\n",
        "\n",
        "        avg_loss += loss.item()\n",
        "        corrects += (torch.max(logit, 1)\n",
        "                     [1].view(target.size()).data == target.data).sum()\n",
        "        epoch_f1 += f1\n",
        "\n",
        "    size = len(data_iter.dataset)\n",
        "    avg_loss /= size\n",
        "    accuracy = 100.0 * corrects/size\n",
        "    print('\\nEvaluation - loss: {:.6f}  acc: {:.4f}%({}/{}) \\n'.format(avg_loss, \n",
        "                                                                       accuracy, \n",
        "                                                                       corrects, \n",
        "                                                                       size))\n",
        "    print(f'\\t F1 score is {epoch_f1/len(data_iter)}')\n",
        "    return accuracy, epoch_f1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hb1x2CTFF5f8",
        "colab_type": "code",
        "outputId": "439cafef-7219-48d6-db80-bde1d8500ab8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 434
        }
      },
      "source": [
        "from torch import optim\n",
        "\n",
        "embed_num = vocab_size\n",
        "embed_dim = 128\n",
        "class_num = 3\n",
        "kernel_num = 100\n",
        "kernel_sizes = [3,4,5]\n",
        "batch_size = 64\n",
        "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "class MyIter:\n",
        "    def __init__(self, it):\n",
        "        self.it = it\n",
        "    def __iter__(self):\n",
        "        for batch in self.it:\n",
        "            yield (batch.reviewText, batch.overall.unsqueeze(1))\n",
        "    def __len__(self):\n",
        "        return len(self.it)\n",
        "\n",
        "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
        "    (train_ds, valid_ds, test_ds), \n",
        "    batch_size=batch_size,\n",
        "    device=device,\n",
        "    sort_key=lambda x: len(x.reviewText),\n",
        "    sort_within_batch=True)\n",
        "\n",
        "\n",
        "cnn = CNN_Text(embed_num,embed_dim,class_num,kernel_num,kernel_sizes)\n",
        "train(train_iterator, valid_iterator, cnn)\n",
        "#train_iterator, valid_iterator, test_iterator = create_iterator(train_ds, valid_ds, test_ds, batch_size, device)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  4%|▎         | 99/2809 [00:22<09:58,  4.53it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\rBatch[100] - loss: 1.104267  acc: 42.1875%(27/64)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/_reduction.py:43: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
            "  warnings.warn(warning.format(ret))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-4aeaba885b30>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0mcnn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCNN_Text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membed_num\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0membed_dim\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mclass_num\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mkernel_num\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mkernel_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_iterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_iterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcnn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;31m#train_iterator, valid_iterator, test_iterator = create_iterator(train_ds, valid_ds, test_ds, batch_size, device)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-ce78377c60e0>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(train_iter, dev_iter, model)\u001b[0m\n\u001b[1;32m     38\u001b[0m                                                                              batch.batch_size))\n\u001b[1;32m     39\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0msteps\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m                 \u001b[0mdev_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_f1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdev_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mdev_acc\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mbest_acc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m                     \u001b[0mbest_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdev_acc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-ce78377c60e0>\u001b[0m in \u001b[0;36meval\u001b[0;34m(data_iter, model)\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0mlogit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize_average\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m         \u001b[0mf1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf1_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moverall\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'macro'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ED4Z5iTRGw_Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(f\"FINAL OUPUT IS \\n{eval(test_iterator,cnn)}\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xxbA9oS_HRVk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "az2uey3AX01Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}