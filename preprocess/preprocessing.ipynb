{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.3-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "import preprocessor as p\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data\n",
    "\n",
    "- Read in the data and then shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-b813b1165b82>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mfile_num\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{file_num}.csv\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mkeep_default_na\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'reviewText'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontains\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\.jpg|\\.png|\\.jpeg|\\.tiff|\\.gif|\\.bmp|\\.heif\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mna\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mold\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreviewText\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deepLearning/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    674\u001b[0m         )\n\u001b[1;32m    675\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 676\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deepLearning/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 448\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    449\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deepLearning/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    878\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 880\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    881\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deepLearning/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1112\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1113\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1114\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1115\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1116\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deepLearning/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1889\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1891\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1892\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1893\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] File 0.csv does not exist: '0.csv'"
     ],
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File 0.csv does not exist: '0.csv'",
     "output_type": "error"
    }
   ],
   "source": [
    "file_num = 0\n",
    "df = pd.read_csv(f\"{file_num}.csv\",keep_default_na=False)\n",
    "df = df[~df['reviewText'].str.contains(\"\\.jpg|\\.png|\\.jpeg|\\.tiff|\\.gif|\\.bmp|\\.heif\", regex=True, na=False)]\n",
    "old = df.copy()\n",
    "text = df.reviewText\n",
    "num_entries = len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Initial Data cleaning\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def tweet_clean(num_entries):\n",
    "    for index, review in tqdm(enumerate(text[0:num_entries])):\n",
    "        clean_text = p.clean(review)\n",
    "        df.reviewText[index] = clean_text"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#tweet_clean(num_entries)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# Helper function to print tokens in a sentence \n",
    "\n",
    "def print_tokens(doc):\n",
    "    for token in doc:\n",
    "        token_text = token.text\n",
    "        token_pos = token.pos_\n",
    "        token_dep = token.dep_\n",
    "        token_vec = token.vector\n",
    "        print(\"{:<12}{:<10}{:<10}\".format(token_text, token_pos, token_dep))\n",
    "    print(token.vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You need to download the spacy model with:\n",
    "\n",
    "python -m spacy download en_core_web_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_md\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean Text\n",
    "\n",
    "1. Pronouns are removed from the text - lemmtaisation\n",
    "2. Then stopwords and punctuation are removed\n",
    "3. Then the removal of spaces and numbers and undetected punctuation\n",
    "4. Removal of URLS and Emails\n",
    "5. The whole sentence is then spell checked and corrected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def tokenizer(sentence):\n",
    "    punctuation = string.punctuation\n",
    "    stopwords = list(STOP_WORDS)\n",
    "    filterTokens = [ word for word in sentence if word.lemma_ != \"-PRON-\" ]\n",
    "    filterTokens = [ word for word in filterTokens if not word.is_stop]\n",
    "    filterTokens = [word for word in filterTokens if word.text not in punctuation ]\n",
    "    filterTokens = [ word for word in filterTokens if word.pos_ not in [\"SPACE\",\"PUNCT\",\"NUM\"]]\n",
    "    filterTokens = [ word for word in filterTokens if not word.like_email ]\n",
    "    return filterTokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def clean_text(num_entries):\n",
    "    for index, review in tqdm(enumerate(text[0:num_entries])):\n",
    "        doc = nlp(review)\n",
    "        clean_token = tokenizer(doc)\n",
    "        sentence = ' '.join([x.text.lower() for x in clean_token])\n",
    "        df.reviewText[index] = sentence\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean all the text and put into a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "text": [
      "\n",
      "0it [00:00, ?it/s]\u001b[A/home/alex/anaconda3/envs/deepLearning/lib/python3.7/site-packages/ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "\n",
      "2it [00:00, 14.66it/s]\u001b[A\n",
      "6it [00:00, 17.47it/s]\u001b[A\n",
      "9it [00:00, 19.43it/s]\u001b[A\n",
      "13it [00:00, 22.52it/s]\u001b[A\n",
      "16it [00:00, 22.92it/s]\u001b[A\n",
      "20it [00:00, 23.91it/s]\u001b[A\n",
      "23it [00:00, 24.21it/s]\u001b[A\n",
      "26it [00:00, 24.92it/s]\u001b[A\n",
      "29it [00:01, 23.93it/s]\u001b[A\n",
      "33it [00:01, 26.86it/s]\u001b[A\n",
      "36it [00:01, 22.71it/s]\u001b[A\n",
      "40it [00:01, 24.15it/s]\u001b[A\n",
      "43it [00:01, 22.99it/s]\u001b[A\n",
      "46it [00:01, 22.56it/s]\u001b[A\n",
      "49it [00:01, 22.19it/s]\u001b[A\n",
      "53it [00:02, 24.66it/s]\u001b[A\n",
      "56it [00:02, 24.54it/s]\u001b[A\n",
      "59it [00:02, 25.57it/s]\u001b[A\n",
      "62it [00:02, 22.47it/s]\u001b[A\n",
      "66it [00:02, 24.24it/s]\u001b[A\n",
      "69it [00:02, 24.17it/s]\u001b[A\n",
      "72it [00:02, 20.75it/s]\u001b[A\n",
      "75it [00:03, 20.79it/s]\u001b[A\n",
      "78it [00:03, 21.94it/s]\u001b[A\n",
      "81it [00:03, 22.73it/s]\u001b[A\n",
      "84it [00:03, 21.73it/s]\u001b[A\n",
      "87it [00:03, 22.69it/s]\u001b[A\n",
      "90it [00:03, 23.44it/s]\u001b[A\n",
      "93it [00:03, 23.24it/s]\u001b[A\n",
      "96it [00:04, 20.96it/s]\u001b[A\n",
      "99it [00:04, 21.59it/s]\u001b[A\n",
      "102it [00:04, 22.46it/s]\u001b[A\n",
      "106it [00:04, 22.29it/s]\u001b[A\n",
      "109it [00:04, 21.98it/s]\u001b[A\n",
      "112it [00:04, 22.99it/s]\u001b[A\n",
      "115it [00:04, 21.51it/s]\u001b[A\n",
      "118it [00:05, 22.18it/s]\u001b[A\n",
      "122it [00:05, 24.86it/s]\u001b[A\n",
      "125it [00:05, 21.49it/s]\u001b[A\n",
      "128it [00:05, 19.20it/s]\u001b[A\n",
      "132it [00:05, 21.44it/s]\u001b[A\n",
      "135it [00:05, 23.29it/s]\u001b[A\n",
      "138it [00:05, 24.13it/s]\u001b[A\n",
      "141it [00:05, 25.31it/s]\u001b[A\n",
      "144it [00:06, 23.35it/s]\u001b[A\n",
      "147it [00:06, 24.14it/s]\u001b[A\n",
      "150it [00:06, 22.12it/s]\u001b[A\n",
      "154it [00:06, 25.02it/s]\u001b[A\n",
      "157it [00:06, 25.82it/s]\u001b[A\n",
      "160it [00:06, 26.75it/s]\u001b[A\n",
      "163it [00:06, 27.54it/s]\u001b[A\n",
      "166it [00:06, 26.05it/s]\u001b[A\n",
      "170it [00:07, 28.22it/s]\u001b[A\n",
      "174it [00:07, 30.20it/s]\u001b[A\n",
      "178it [00:07, 30.28it/s]\u001b[A\n",
      "182it [00:07, 31.03it/s]\u001b[A\n",
      "186it [00:07, 31.43it/s]\u001b[A\n",
      "190it [00:07, 28.57it/s]\u001b[A\n",
      "194it [00:07, 30.24it/s]\u001b[A\n",
      "198it [00:07, 30.86it/s]\u001b[A\n",
      "202it [00:08, 31.70it/s]\u001b[A\n",
      "206it [00:08, 30.74it/s]\u001b[A\n",
      "210it [00:08, 32.15it/s]\u001b[A\n",
      "214it [00:08, 29.35it/s]\u001b[A\n",
      "218it [00:08, 31.41it/s]\u001b[A\n",
      "222it [00:08, 31.65it/s]\u001b[A\n",
      "226it [00:08, 26.48it/s]\u001b[A\n",
      "230it [00:09, 28.41it/s]\u001b[A\n",
      "234it [00:09, 26.06it/s]\u001b[A\n",
      "237it [00:09, 26.21it/s]\u001b[A\n",
      "241it [00:09, 28.45it/s]\u001b[A\n",
      "244it [00:09, 24.42it/s]\u001b[A\n",
      "247it [00:09, 24.13it/s]\u001b[A\n",
      "251it [00:09, 26.21it/s]\u001b[A\n",
      "255it [00:09, 27.86it/s]\u001b[A\n",
      "258it [00:10, 26.27it/s]\u001b[A\n",
      "261it [00:10, 26.93it/s]\u001b[A\n",
      "265it [00:10, 28.83it/s]\u001b[A\n",
      "268it [00:10, 28.87it/s]\u001b[A\n",
      "271it [00:10, 24.82it/s]\u001b[A\n",
      "275it [00:10, 27.90it/s]\u001b[A\n",
      "279it [00:10, 27.18it/s]\u001b[A\n",
      "282it [00:10, 27.08it/s]\u001b[A\n",
      "286it [00:11, 29.32it/s]\u001b[A\n",
      "290it [00:11, 29.45it/s]\u001b[A\n",
      "294it [00:11, 29.05it/s]\u001b[A\n",
      "298it [00:11, 30.89it/s]\u001b[A\n",
      "302it [00:11, 28.75it/s]\u001b[A\n",
      "305it [00:11, 27.22it/s]\u001b[A\n",
      "308it [00:11, 27.37it/s]\u001b[A\n",
      "311it [00:11, 27.44it/s]\u001b[A\n",
      "314it [00:12, 26.87it/s]\u001b[A\n",
      "318it [00:12, 29.38it/s]\u001b[A\n",
      "322it [00:12, 29.73it/s]\u001b[A\n",
      "326it [00:12, 29.46it/s]\u001b[A\n",
      "330it [00:12, 30.90it/s]\u001b[A\n",
      "334it [00:12, 29.41it/s]\u001b[A\n",
      "337it [00:12, 29.36it/s]\u001b[A\n",
      "341it [00:12, 30.55it/s]\u001b[A\n",
      "345it [00:13, 27.89it/s]\u001b[A\n",
      "348it [00:13, 27.65it/s]\u001b[A\n",
      "352it [00:13, 28.59it/s]\u001b[A\n",
      "356it [00:13, 30.21it/s]\u001b[A\n",
      "360it [00:13, 28.74it/s]\u001b[A\n",
      "363it [00:13, 24.74it/s]\u001b[A\n",
      "366it [00:13, 24.99it/s]\u001b[A\n",
      "370it [00:14, 27.32it/s]\u001b[A\n",
      "374it [00:14, 28.29it/s]\u001b[A\n",
      "378it [00:14, 30.04it/s]\u001b[A\n",
      "382it [00:14, 28.35it/s]\u001b[A\n",
      "385it [00:14, 27.62it/s]\u001b[A\n",
      "389it [00:14, 29.66it/s]\u001b[A\n",
      "393it [00:14, 26.54it/s]\u001b[A\n",
      "396it [00:15, 23.68it/s]\u001b[A\n",
      "399it [00:15, 24.28it/s]\u001b[A\n",
      "403it [00:15, 27.04it/s]\u001b[A\n",
      "406it [00:15, 26.98it/s]\u001b[A\n",
      "410it [00:15, 28.30it/s]\u001b[A\n",
      "414it [00:15, 30.06it/s]\u001b[A\n",
      "418it [00:15, 30.82it/s]\u001b[A\n",
      "422it [00:15, 29.02it/s]\u001b[A\n",
      "425it [00:15, 26.79it/s]\u001b[A\n",
      "428it [00:16, 18.49it/s]\u001b[A\n",
      "431it [00:16, 19.32it/s]\u001b[A\n",
      "434it [00:16, 20.30it/s]\u001b[A\n",
      "437it [00:16, 20.88it/s]\u001b[A\n",
      "440it [00:16, 21.29it/s]\u001b[A\n",
      "443it [00:16, 22.44it/s]\u001b[A\n",
      "446it [00:17, 19.98it/s]\u001b[A\n",
      "449it [00:17, 21.90it/s]\u001b[A\n",
      "453it [00:17, 24.91it/s]\u001b[A\n",
      "457it [00:17, 26.23it/s]\u001b[A\n",
      "460it [00:17, 25.34it/s]\u001b[A\n",
      "463it [00:17, 26.54it/s]\u001b[A\n",
      "467it [00:17, 27.17it/s]\u001b[A\n",
      "471it [00:17, 28.43it/s]\u001b[A"
     ],
     "output_type": "stream"
    },
    {
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-00ca84ee62ae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mclean_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_entries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-4d561f36f5d3>\u001b[0m in \u001b[0;36mclean_text\u001b[0;34m(num_entries)\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mclean_token\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0msentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mclean_token\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreviewText\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deepLearning/lib/python3.7/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   1043\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1044\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcacher_needs_updating\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1045\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_update_cacher\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1046\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1047\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_set_with_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deepLearning/lib/python3.7/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_maybe_update_cacher\u001b[0;34m(self, clear, verify_is_copy)\u001b[0m\n\u001b[1;32m   3280\u001b[0m                 \u001b[0;31m#  case where it will raise.  (Uh, not clear why)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3281\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3282\u001b[0;31m                     \u001b[0mref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cache_changed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcacher\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3283\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mAssertionError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3284\u001b[0m                     \u001b[0;31m# ref._data.setitem can raise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deepLearning/lib/python3.7/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_maybe_cache_changed\u001b[0;34m(self, item, value)\u001b[0m\n\u001b[1;32m   3239\u001b[0m         \"\"\"The object has called back to us saying maybe it has changed.\n\u001b[1;32m   3240\u001b[0m         \"\"\"\n\u001b[0;32m-> 3241\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3243\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deepLearning/lib/python3.7/site-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mset\u001b[0;34m(self, item, value)\u001b[0m\n\u001b[1;32m   1088\u001b[0m             \u001b[0mblk_locs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mblklocs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mval_locs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1089\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mblk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_store\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1090\u001b[0;31m                 \u001b[0mblk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblk_locs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue_getitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_locs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1091\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1092\u001b[0m                 \u001b[0munfit_mgr_locs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmgr_locs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_array\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mblk_locs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deepLearning/lib/python3.7/site-packages/pandas/core/internals/blocks.py\u001b[0m in \u001b[0;36mset\u001b[0;34m(self, locs, values)\u001b[0m\n\u001b[1;32m    374\u001b[0m         \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m         \"\"\"\n\u001b[0;32m--> 376\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlocs\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    377\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdelete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ],
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error"
    }
   ],
   "source": [
    "clean_text(num_entries)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Save the cleaned data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df.to_csv(f'kindle_reviews_cleaned_{file_num}.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# Just to test \n",
    "\n",
    "# twitter - 62 ops/sec - 3 hours\n",
    "# spacy clean - 30 ops/sec - 8 hours\n",
    "\n",
    "# 11 hours\n",
    "\n",
    "# 6 threads = 11/6 = 1.86 hours\n",
    "\n",
    "df_2 = pd.read_csv(\"data_split/kindle_reviews_cleaned_0.csv\")\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "\n",
    "# def index_marks(nrows, chunk_size):\n",
    "#     return range(chunk_size, math.ceil(nrows / chunk_size) * chunk_size, chunk_size)\n",
    "# \n",
    "# \n",
    "# def split(dfm, chunk_size):\n",
    "#     indices = index_marks(dfm.shape[0], chunk_size)\n",
    "#     return np.split(dfm, indices)\n",
    "# \n",
    "# chunks = split(df, 163765)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# for inx, c in enumerate(chunks):\n",
    "#     c.to_csv(f\"data_split/{inx}.csv\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../kindle_reviews_cleaned.csv\")\n",
    "df = df.dropna()\n",
    "text = df.reviewText"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def tokenizer_2(sentence):\n",
    "    stopwords = list(STOP_WORDS)\n",
    "    filterTokens = [word for word in sentence if word not in stopwords ]\n",
    "    return filterTokens"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def clean_text_2(num_entries):\n",
    "    for index, review in tqdm(enumerate(text[0:num_entries])):\n",
    "        if type(review) is str:\n",
    "            clean_token = tokenizer_2(list(review.split()))\n",
    "            sentence = ' '.join(clean_token)\n",
    "            print(sentence)\n",
    "            #df.reviewText[index] = sentence\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stderr",
     "text": [
      "0it [00:00, ?it/s]/home/alex/anaconda3/envs/deepLearning/lib/python3.7/site-packages/ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "50it [00:00, 63.95it/s]\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "num_entries = len(df)\n",
    "clean_text_2(50)\n",
    "df.to_csv(\"cleaned_again.csv\", index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "['free',\n 'considered',\n 'book',\n 'title',\n 'little',\n 'knew',\n 'gone',\n 'gut',\n 'pay',\n 'money',\n 'waste',\n 'time',\n 'reading',\n 'barely',\n 'plot',\n 'amazed',\n 'mean',\n 'guy',\n 'new',\n 'boss',\n 'maybe',\n 'let',\n 'slide',\n 'madly',\n 'love',\n 'lust',\n 'guy',\n 'friend',\n 'wade',\n 'break',\n 'description',\n 'guy',\n 'want',\n 'gag',\n 'things',\n 'truly',\n 'case',\n 'pay',\n 'case',\n 'paid',\n 'actually',\n 'spend',\n 'money',\n 'trash',\n 'know',\n 'takes',\n 'hour',\n 'read',\n 'end',\n 'enjoying',\n 'book',\n 'think',\n 'willing',\n 'spend',\n 'hours',\n 'worth',\n 'entertainment']"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 8
    }
   ],
   "source": [
    " \n",
    "x = \"free considered book title little knew gone gut pay money waste time reading there barely plot amazed mean guy new boss maybe let slide madly love lust guy but friend wade give break just description guy want gag things this truly case pay case paid for actually spend money trash know takes hour read end enjoying book think willing spend hours worth entertainment\"\n",
    "tokenizer_2(list(x.split()))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ]
}